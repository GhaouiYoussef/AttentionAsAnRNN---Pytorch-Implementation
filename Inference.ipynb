{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.0208, -0.0104,  0.0255, -0.0144, -0.0352, -0.0093, -0.1045, -0.0262,\n",
      "          0.0012,  0.0876, -0.0329, -0.0573,  0.0362, -0.0485,  0.0376, -0.0534,\n",
      "         -0.0725,  0.0190, -0.0151, -0.0662, -0.1108,  0.0521,  0.0771,  0.1231,\n",
      "          0.0989,  0.0649, -0.0252,  0.0742, -0.0007, -0.0008,  0.0393,  0.0540,\n",
      "         -0.0276, -0.0059, -0.0349,  0.0800, -0.0888,  0.0413,  0.0895, -0.0748]])\n",
      "c_k (normalization term):\n",
      " tensor([[0.0752]])\n",
      "m_k (max score):\n",
      " tensor([[0.]])\n",
      "--------------------------------------------------\n",
      "Frame 2:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.0175, -0.0087,  0.0243, -0.0116, -0.0328, -0.0033, -0.0994, -0.0298,\n",
      "          0.0064,  0.0942, -0.0316, -0.0524,  0.0383, -0.0468,  0.0425, -0.0486,\n",
      "         -0.0831,  0.0199, -0.0154, -0.0665, -0.1138,  0.0535,  0.0723,  0.1237,\n",
      "          0.1064,  0.0601, -0.0248,  0.0722,  0.0067,  0.0020,  0.0354,  0.0522,\n",
      "         -0.0248, -0.0048, -0.0353,  0.0746, -0.0931,  0.0452,  0.0796, -0.0784]])\n",
      "c_k (normalization term):\n",
      " tensor([[0.0794]])\n",
      "m_k (max score):\n",
      " tensor([[0.]])\n",
      "--------------------------------------------------\n",
      "Frame 3:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.1442,  0.1644,  0.0271, -0.0985,  0.1937,  0.0368,  0.0948, -0.1111,\n",
      "         -0.1019, -0.1466, -0.2087,  0.2310,  0.1049, -0.1124, -0.0423,  0.1147,\n",
      "         -0.2988,  0.0906,  0.0706, -0.3862, -0.2146,  0.3864, -0.0705,  0.1326,\n",
      "         -0.0857, -0.0465, -0.1525,  0.3777, -0.3458, -0.1642, -0.0024, -0.0050,\n",
      "          0.0098, -0.0104, -0.1047,  0.0325, -0.1178,  0.1198,  0.0064, -0.1198]])\n",
      "c_k (normalization term):\n",
      " tensor([[0.2254]])\n",
      "m_k (max score):\n",
      " tensor([[0.]])\n",
      "--------------------------------------------------\n",
      "Frame 4:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 7.5134e-02,  1.2853e+00,  1.6589e+00, -1.6491e+00, -7.4586e-01,\n",
      "         -5.5980e-01, -3.3788e+00,  4.3267e-01, -4.4363e-01,  4.7734e-01,\n",
      "         -1.1769e+00,  1.1796e+00, -6.1127e-01, -5.4230e-01,  2.5773e-01,\n",
      "         -2.6657e+00, -1.0373e+00, -1.1433e+00, -2.0838e+00, -1.6192e-01,\n",
      "         -8.3218e-01, -4.0723e-01,  1.7571e+00,  2.8214e-01, -6.8773e-01,\n",
      "          8.1191e-01, -9.9974e-01,  6.5892e-01,  1.4043e+00, -9.1216e-01,\n",
      "         -5.1418e-01, -9.2297e-01,  1.6792e+00, -2.8796e-01, -8.7696e-01,\n",
      "         -1.3238e+00,  2.5517e-01, -2.8685e-03,  3.2482e-01, -1.6686e-01]])\n",
      "c_k (normalization term):\n",
      " tensor([[1.0000]])\n",
      "m_k (max score):\n",
      " tensor([[14.3556]])\n",
      "--------------------------------------------------\n",
      "Frame 5:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.6902,  0.7747,  0.7031, -0.6891,  0.3738,  0.5395, -3.8021,  1.9026,\n",
      "         -0.1743,  0.1636, -1.4535,  0.1693, -0.7693, -0.9708, -1.6498, -0.5501,\n",
      "         -1.7073, -0.3665,  0.3344, -1.0647,  1.6080,  1.4189, -0.1415,  0.7397,\n",
      "         -1.0218,  2.2530,  1.2688, -0.1090,  0.3330, -0.1917, -1.5908,  1.1476,\n",
      "          0.4793, -2.0211, -0.4821,  0.2303, -0.3286, -0.2998,  1.6199, -1.4386]])\n",
      "c_k (normalization term):\n",
      " tensor([[1.3469]])\n",
      "m_k (max score):\n",
      " tensor([[15.4143]])\n",
      "--------------------------------------------------\n",
      "Frame 6:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.6902,  0.7747,  0.7030, -0.6891,  0.3738,  0.5395, -3.8021,  1.9026,\n",
      "         -0.1743,  0.1636, -1.4535,  0.1693, -0.7694, -0.9708, -1.6498, -0.5500,\n",
      "         -1.7073, -0.3665,  0.3344, -1.0647,  1.6080,  1.4188, -0.1415,  0.7398,\n",
      "         -1.0218,  2.2530,  1.2688, -0.1090,  0.3330, -0.1918, -1.5908,  1.1476,\n",
      "          0.4793, -2.0211, -0.4822,  0.2303, -0.3286, -0.2998,  1.6199, -1.4385]])\n",
      "c_k (normalization term):\n",
      " tensor([[1.3469]])\n",
      "m_k (max score):\n",
      " tensor([[15.4143]])\n",
      "--------------------------------------------------\n",
      "Frame 7:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.6902,  0.7747,  0.7030, -0.6891,  0.3738,  0.5395, -3.8021,  1.9026,\n",
      "         -0.1743,  0.1636, -1.4535,  0.1693, -0.7694, -0.9708, -1.6498, -0.5500,\n",
      "         -1.7073, -0.3665,  0.3344, -1.0647,  1.6080,  1.4188, -0.1415,  0.7398,\n",
      "         -1.0218,  2.2530,  1.2688, -0.1090,  0.3330, -0.1918, -1.5908,  1.1476,\n",
      "          0.4793, -2.0211, -0.4822,  0.2303, -0.3286, -0.2998,  1.6199, -1.4385]])\n",
      "c_k (normalization term):\n",
      " tensor([[1.3469]])\n",
      "m_k (max score):\n",
      " tensor([[15.4143]])\n",
      "--------------------------------------------------\n",
      "Frame 8:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.6902,  0.7747,  0.7030, -0.6891,  0.3738,  0.5395, -3.8021,  1.9026,\n",
      "         -0.1743,  0.1636, -1.4535,  0.1693, -0.7694, -0.9708, -1.6498, -0.5500,\n",
      "         -1.7073, -0.3665,  0.3344, -1.0647,  1.6080,  1.4188, -0.1415,  0.7398,\n",
      "         -1.0218,  2.2530,  1.2688, -0.1090,  0.3330, -0.1918, -1.5908,  1.1476,\n",
      "          0.4793, -2.0211, -0.4822,  0.2303, -0.3286, -0.2998,  1.6199, -1.4385]])\n",
      "c_k (normalization term):\n",
      " tensor([[1.3469]])\n",
      "m_k (max score):\n",
      " tensor([[15.4143]])\n",
      "--------------------------------------------------\n",
      "Frame 9:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.6902,  0.7747,  0.7030, -0.6891,  0.3738,  0.5395, -3.8021,  1.9026,\n",
      "         -0.1743,  0.1636, -1.4535,  0.1693, -0.7694, -0.9708, -1.6498, -0.5500,\n",
      "         -1.7073, -0.3665,  0.3344, -1.0647,  1.6080,  1.4188, -0.1415,  0.7398,\n",
      "         -1.0218,  2.2530,  1.2688, -0.1090,  0.3330, -0.1918, -1.5908,  1.1476,\n",
      "          0.4793, -2.0211, -0.4822,  0.2303, -0.3286, -0.2998,  1.6199, -1.4385]])\n",
      "c_k (normalization term):\n",
      " tensor([[1.3469]])\n",
      "m_k (max score):\n",
      " tensor([[15.4143]])\n",
      "--------------------------------------------------\n",
      "Frame 10:\n",
      "a_k (weighted sum of values):\n",
      " tensor([[ 0.6902,  0.7747,  0.7030, -0.6891,  0.3738,  0.5395, -3.8021,  1.9026,\n",
      "         -0.1743,  0.1635, -1.4535,  0.1693, -0.7694, -0.9708, -1.6498, -0.5500,\n",
      "         -1.7073, -0.3664,  0.3344, -1.0647,  1.6080,  1.4189, -0.1415,  0.7398,\n",
      "         -1.0219,  2.2530,  1.2688, -0.1090,  0.3330, -0.1917, -1.5908,  1.1476,\n",
      "          0.4793, -2.0211, -0.4822,  0.2303, -0.3286, -0.2998,  1.6199, -1.4386]])\n",
      "c_k (normalization term):\n",
      " tensor([[1.3469]])\n",
      "m_k (max score):\n",
      " tensor([[15.4143]])\n",
      "--------------------------------------------------\n",
      "Final Attention Output:\n",
      " tensor([[ 0.5124,  0.5751,  0.5220, -0.5116,  0.2775,  0.4006, -2.8228,  1.4125,\n",
      "         -0.1294,  0.1214, -1.0791,  0.1257, -0.5712, -0.7207, -1.2248, -0.4084,\n",
      "         -1.2675, -0.2721,  0.2482, -0.7905,  1.1938,  1.0534, -0.1050,  0.5492,\n",
      "         -0.7586,  1.6727,  0.9420, -0.0809,  0.2472, -0.1424, -1.1810,  0.8520,\n",
      "          0.3558, -1.5005, -0.3580,  0.1710, -0.2440, -0.2226,  1.2027, -1.0680]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the AttentionRNNCell (from your implementation)\n",
    "class AttentionRNNCell(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionRNNCell, self).__init__()\n",
    "        self.d_model = d_model  # Dimensionality of keys/queries/values\n",
    "\n",
    "    def forward_iterative(self, q, k, v, prev_a=None, prev_c=None, prev_m=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Initialize previous states if not provided\n",
    "        if prev_a is None:\n",
    "            prev_a = torch.zeros(batch_size, self.d_model).to(k.device)\n",
    "        if prev_c is None:\n",
    "            prev_c = torch.zeros(batch_size, 1).to(k.device)\n",
    "        if prev_m is None:\n",
    "            prev_m = torch.full((batch_size, 1), 0).to(k.device)\n",
    "\n",
    "        # Compute scores s_k = q . k^T\n",
    "        s_k = torch.sum(q * k, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update m_k (max cumulative score)\n",
    "        m_k = torch.max(s_k, prev_m)\n",
    "\n",
    "        # Compute exp terms for stability\n",
    "        exp_term1 = torch.exp(prev_m - m_k)  # Shape: (batch_size, 1)\n",
    "        exp_term2 = torch.exp(s_k - m_k)    # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update a_k and c_k\n",
    "        a_k = prev_a * exp_term1 + v * exp_term2  # Shape: (batch_size, d_model)\n",
    "        c_k = prev_c * exp_term1 + exp_term2      # Shape: (batch_size, 1)\n",
    "\n",
    "        return a_k, c_k, m_k\n",
    "\n",
    "# Configuration\n",
    "batch_size = 1  # Number of audio sequences to process at once\n",
    "input_dim = 10  # Number of audio frames in the sequence\n",
    "d_model = 40    # Dimensionality of each audio frame (e.g., MFCCs)\n",
    "\n",
    "# Simulate audio frames (e.g., MFCCs or spectrogram features)\n",
    "# Each frame is a vector of size d_model\n",
    "audio_frames = torch.randn(input_dim, batch_size, d_model)  # Shape: (input_dim, batch_size, d_model)\n",
    "\n",
    "# Initialize a fixed query vector (learned during training)\n",
    "query = torch.randn(batch_size, d_model)  # Shape: (batch_size, d_model)\n",
    "\n",
    "# Initialize AttentionRNNCell\n",
    "cell = AttentionRNNCell(d_model)\n",
    "\n",
    "# Initialize hidden states\n",
    "a_k, c_k, m_k = None, None, None\n",
    "\n",
    "# Process audio frames iteratively (simulating a streaming scenario)\n",
    "for i in range(input_dim):\n",
    "    # Get the current frame (key and value)\n",
    "    frame = audio_frames[i, :, :]  # Shape: (batch_size, d_model)\n",
    "\n",
    "    # Perform a step of attention\n",
    "    a_k, c_k, m_k = cell.forward_iterative(query, frame, frame, a_k, c_k, m_k)\n",
    "\n",
    "    # Print intermediate results (optional)\n",
    "    print(f\"Frame {i+1}:\")\n",
    "    print(\"a_k (weighted sum of values):\\n\", a_k)\n",
    "    print(\"c_k (normalization term):\\n\", c_k)\n",
    "    print(\"m_k (max score):\\n\", m_k)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Final attention output\n",
    "attention_output = a_k / c_k  # Shape: (batch_size, d_model)\n",
    "print(\"Final Attention Output:\\n\", attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [10/32], Loss: 1.7103, Accuracy: 19.69%\n",
      "Epoch [1/10], Batch [20/32], Loss: 1.7789, Accuracy: 16.88%\n",
      "Epoch [1/10], Batch [30/32], Loss: 1.7023, Accuracy: 21.56%\n",
      "Epoch [2/10], Batch [10/32], Loss: 1.6910, Accuracy: 19.06%\n",
      "Epoch [2/10], Batch [20/32], Loss: 1.6731, Accuracy: 25.00%\n",
      "Epoch [2/10], Batch [30/32], Loss: 1.6674, Accuracy: 20.00%\n",
      "Epoch [3/10], Batch [10/32], Loss: 1.7264, Accuracy: 16.25%\n",
      "Epoch [3/10], Batch [20/32], Loss: 1.6854, Accuracy: 19.06%\n",
      "Epoch [3/10], Batch [30/32], Loss: 1.6568, Accuracy: 21.88%\n",
      "Epoch [4/10], Batch [10/32], Loss: 1.6798, Accuracy: 20.00%\n",
      "Epoch [4/10], Batch [20/32], Loss: 1.6567, Accuracy: 20.31%\n",
      "Epoch [4/10], Batch [30/32], Loss: 1.6862, Accuracy: 21.56%\n",
      "Epoch [5/10], Batch [10/32], Loss: 1.6710, Accuracy: 23.12%\n",
      "Epoch [5/10], Batch [20/32], Loss: 1.6683, Accuracy: 20.94%\n",
      "Epoch [5/10], Batch [30/32], Loss: 1.6650, Accuracy: 20.00%\n",
      "Epoch [6/10], Batch [10/32], Loss: 1.6069, Accuracy: 29.38%\n",
      "Epoch [6/10], Batch [20/32], Loss: 1.6358, Accuracy: 21.88%\n",
      "Epoch [6/10], Batch [30/32], Loss: 1.6311, Accuracy: 24.06%\n",
      "Epoch [7/10], Batch [10/32], Loss: 1.6635, Accuracy: 17.81%\n",
      "Epoch [7/10], Batch [20/32], Loss: 1.6488, Accuracy: 19.69%\n",
      "Epoch [7/10], Batch [30/32], Loss: 1.6421, Accuracy: 20.00%\n",
      "Epoch [8/10], Batch [10/32], Loss: 1.6401, Accuracy: 21.25%\n",
      "Epoch [8/10], Batch [20/32], Loss: 1.6533, Accuracy: 20.00%\n",
      "Epoch [8/10], Batch [30/32], Loss: 1.6555, Accuracy: 19.06%\n",
      "Epoch [9/10], Batch [10/32], Loss: 1.6783, Accuracy: 20.94%\n",
      "Epoch [9/10], Batch [20/32], Loss: 1.6414, Accuracy: 20.31%\n",
      "Epoch [9/10], Batch [30/32], Loss: 1.6410, Accuracy: 24.38%\n",
      "Epoch [10/10], Batch [10/32], Loss: 1.6241, Accuracy: 23.44%\n",
      "Epoch [10/10], Batch [20/32], Loss: 1.6487, Accuracy: 16.88%\n",
      "Epoch [10/10], Batch [30/32], Loss: 1.5944, Accuracy: 22.19%\n",
      "Test Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the AttentionRNNCell (from your implementation)\n",
    "class AttentionRNNCell(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionRNNCell, self).__init__()\n",
    "        self.d_model = d_model  # Dimensionality of keys/queries/values\n",
    "\n",
    "    def forward_iterative(self, q, k, v, prev_a=None, prev_c=None, prev_m=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Initialize previous states if not provided\n",
    "        if prev_a is None:\n",
    "            prev_a = torch.zeros(batch_size, self.d_model).to(k.device)\n",
    "        if prev_c is None:\n",
    "            prev_c = torch.zeros(batch_size, 1).to(k.device)\n",
    "        if prev_m is None:\n",
    "            prev_m = torch.full((batch_size, 1), 0).to(k.device)\n",
    "\n",
    "        # Compute scores s_k = q . k^T\n",
    "        s_k = torch.sum(q * k, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update m_k (max cumulative score)\n",
    "        m_k = torch.max(s_k, prev_m)\n",
    "\n",
    "        # Compute exp terms for stability\n",
    "        exp_term1 = torch.exp(prev_m - m_k)  # Shape: (batch_size, 1)\n",
    "        exp_term2 = torch.exp(s_k - m_k)    # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update a_k and c_k\n",
    "        a_k = prev_a * exp_term1 + v * exp_term2  # Shape: (batch_size, d_model)\n",
    "        c_k = prev_c * exp_term1 + exp_term2      # Shape: (batch_size, 1)\n",
    "\n",
    "        return a_k, c_k, m_k\n",
    "\n",
    "# Define the full model\n",
    "class AttentionRNNClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(AttentionRNNClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Attention RNN Cell\n",
    "        self.attention_cell = AttentionRNNCell(d_model)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Initialize query (learnable parameter)\n",
    "        query = nn.Parameter(torch.randn(batch_size, d_model)).to(x.device)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        a_k, c_k, m_k = None, None, None\n",
    "\n",
    "        # Process the sequence iteratively\n",
    "        for i in range(seq_len):\n",
    "            frame = x[:, i, :]  # Get the i-th frame in the sequence\n",
    "            a_k, c_k, m_k = self.attention_cell.forward_iterative(query, frame, frame, a_k, c_k, m_k)\n",
    "\n",
    "        # Final attention output\n",
    "        attention_output = a_k / c_k  # Shape: (batch_size, d_model)\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.fc(attention_output)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Simulate a dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, d_model, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Simulate random audio sequences and labels\n",
    "        self.data = torch.randn(num_samples, seq_len, d_model)  # Random audio frames\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))  # Random labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Configuration\n",
    "batch_size = 32\n",
    "seq_len = 10  # Number of audio frames per sequence\n",
    "d_model = 40  # Dimensionality of each audio frame (e.g., MFCCs)\n",
    "num_classes = 5  # Number of classes for classification\n",
    "num_samples = 1000  # Number of samples in the dataset\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = AudioDataset(num_samples, seq_len, d_model, num_classes)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = AttentionRNNClassifier(d_model, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], \"\n",
    "                  f\"Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_dataset = AudioDataset(num_samples=100, seq_len=seq_len, d_model=d_model, num_classes=num_classes)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        logits = model(data)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.datasets import LIBRISPEECH\n",
    "\n",
    "class LibriSpeechDataset(Dataset):\n",
    "    def __init__(self, root, url):\n",
    "        self.dataset = LIBRISPEECH(root=root, url=url)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = self.dataset[idx]\n",
    "        # Extract MFCCs or other features\n",
    "        mfcc = torchaudio.transforms.MFCC()(waveform)\n",
    "        return mfcc, speaker_id  # Use speaker_id as the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNNClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(AttentionRNNClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Learnable query\n",
    "        self.query = nn.Parameter(torch.randn(1, d_model))\n",
    "\n",
    "        # Attention RNN Cell\n",
    "        self.attention_cell = AttentionRNNCell(d_model)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Expand query to match batch size\n",
    "        query = self.query.expand(batch_size, -1)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        a_k, c_k, m_k = None, None, None\n",
    "\n",
    "        # Process the sequence iteratively\n",
    "        for i in range(seq_len):\n",
    "            frame = x[:, i, :]  # Get the i-th frame in the sequence\n",
    "            a_k, c_k, m_k = self.attention_cell.forward_iterative(query, frame, frame, a_k, c_k, m_k)\n",
    "\n",
    "        # Final attention output\n",
    "        attention_output = a_k / c_k  # Shape: (batch_size, d_model)\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.fc(attention_output)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Add positional encoding to the model\n",
    "class AttentionRNNClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(AttentionRNNClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Learnable query\n",
    "        self.query = nn.Parameter(torch.randn(1, d_model))\n",
    "\n",
    "        # Attention RNN Cell\n",
    "        self.attention_cell = AttentionRNNCell(d_model)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Expand query to match batch size\n",
    "        query = self.query.expand(batch_size, -1)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        a_k, c_k, m_k = None, None, None\n",
    "\n",
    "        # Process the sequence iteratively\n",
    "        for i in range(seq_len):\n",
    "            frame = x[:, i, :]  # Get the i-th frame in the sequence\n",
    "            a_k, c_k, m_k = self.attention_cell.forward_iterative(query, frame, frame, a_k, c_k, m_k)\n",
    "\n",
    "        # Final attention output\n",
    "        attention_output = a_k / c_k  # Shape: (batch_size, d_model)\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.fc(attention_output)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Batch [10/16], Loss: 1.8247, Accuracy: 19.69%\n",
      "Epoch [2/1000], Batch [10/16], Loss: 1.7364, Accuracy: 19.22%\n",
      "Epoch [3/1000], Batch [10/16], Loss: 1.6970, Accuracy: 18.75%\n",
      "Epoch [4/1000], Batch [10/16], Loss: 1.6747, Accuracy: 18.91%\n",
      "Epoch [5/1000], Batch [10/16], Loss: 1.6774, Accuracy: 19.84%\n",
      "Epoch [6/1000], Batch [10/16], Loss: 1.6766, Accuracy: 19.69%\n",
      "Epoch [7/1000], Batch [10/16], Loss: 1.6509, Accuracy: 20.94%\n",
      "Epoch [8/1000], Batch [10/16], Loss: 1.6509, Accuracy: 21.56%\n",
      "Epoch [9/1000], Batch [10/16], Loss: 1.6447, Accuracy: 21.88%\n",
      "Epoch [10/1000], Batch [10/16], Loss: 1.6205, Accuracy: 22.03%\n",
      "Epoch [11/1000], Batch [10/16], Loss: 1.6279, Accuracy: 21.88%\n",
      "Epoch [12/1000], Batch [10/16], Loss: 1.6261, Accuracy: 22.03%\n",
      "Epoch [13/1000], Batch [10/16], Loss: 1.6063, Accuracy: 25.94%\n",
      "Epoch [14/1000], Batch [10/16], Loss: 1.5992, Accuracy: 25.47%\n",
      "Epoch [15/1000], Batch [10/16], Loss: 1.6054, Accuracy: 23.75%\n",
      "Epoch [16/1000], Batch [10/16], Loss: 1.5985, Accuracy: 25.31%\n",
      "Epoch [17/1000], Batch [10/16], Loss: 1.5866, Accuracy: 26.88%\n",
      "Epoch [18/1000], Batch [10/16], Loss: 1.5959, Accuracy: 24.53%\n",
      "Epoch [19/1000], Batch [10/16], Loss: 1.5777, Accuracy: 27.19%\n",
      "Epoch [20/1000], Batch [10/16], Loss: 1.5741, Accuracy: 29.53%\n",
      "Epoch [21/1000], Batch [10/16], Loss: 1.5778, Accuracy: 28.44%\n",
      "Epoch [22/1000], Batch [10/16], Loss: 1.5733, Accuracy: 28.75%\n",
      "Epoch [23/1000], Batch [10/16], Loss: 1.5830, Accuracy: 27.50%\n",
      "Epoch [24/1000], Batch [10/16], Loss: 1.5727, Accuracy: 28.75%\n",
      "Epoch [25/1000], Batch [10/16], Loss: 1.5699, Accuracy: 27.66%\n",
      "Epoch [26/1000], Batch [10/16], Loss: 1.5644, Accuracy: 28.59%\n",
      "Epoch [27/1000], Batch [10/16], Loss: 1.5622, Accuracy: 29.22%\n",
      "Epoch [28/1000], Batch [10/16], Loss: 1.5580, Accuracy: 30.16%\n",
      "Epoch [29/1000], Batch [10/16], Loss: 1.5598, Accuracy: 28.91%\n",
      "Epoch [30/1000], Batch [10/16], Loss: 1.5565, Accuracy: 29.06%\n",
      "Epoch [31/1000], Batch [10/16], Loss: 1.5446, Accuracy: 30.62%\n",
      "Epoch [32/1000], Batch [10/16], Loss: 1.5565, Accuracy: 29.53%\n",
      "Epoch [33/1000], Batch [10/16], Loss: 1.5608, Accuracy: 28.59%\n",
      "Epoch [34/1000], Batch [10/16], Loss: 1.5549, Accuracy: 29.53%\n",
      "Epoch [35/1000], Batch [10/16], Loss: 1.5417, Accuracy: 30.78%\n",
      "Epoch [36/1000], Batch [10/16], Loss: 1.5511, Accuracy: 30.47%\n",
      "Epoch [37/1000], Batch [10/16], Loss: 1.5528, Accuracy: 29.84%\n",
      "Epoch [38/1000], Batch [10/16], Loss: 1.5415, Accuracy: 30.16%\n",
      "Epoch [39/1000], Batch [10/16], Loss: 1.5395, Accuracy: 30.00%\n",
      "Epoch [40/1000], Batch [10/16], Loss: 1.5291, Accuracy: 29.69%\n",
      "Epoch [41/1000], Batch [10/16], Loss: 1.5398, Accuracy: 30.47%\n",
      "Epoch [42/1000], Batch [10/16], Loss: 1.5372, Accuracy: 31.25%\n",
      "Epoch [43/1000], Batch [10/16], Loss: 1.5460, Accuracy: 29.53%\n",
      "Epoch [44/1000], Batch [10/16], Loss: 1.5388, Accuracy: 30.00%\n",
      "Epoch [45/1000], Batch [10/16], Loss: 1.5300, Accuracy: 31.72%\n",
      "Epoch [46/1000], Batch [10/16], Loss: 1.5444, Accuracy: 30.16%\n",
      "Epoch [47/1000], Batch [10/16], Loss: 1.5267, Accuracy: 32.03%\n",
      "Epoch [48/1000], Batch [10/16], Loss: 1.5211, Accuracy: 31.09%\n",
      "Epoch [49/1000], Batch [10/16], Loss: 1.5328, Accuracy: 30.00%\n",
      "Epoch [50/1000], Batch [10/16], Loss: 1.5352, Accuracy: 33.44%\n",
      "Epoch [51/1000], Batch [10/16], Loss: 1.5236, Accuracy: 31.88%\n",
      "Epoch [52/1000], Batch [10/16], Loss: 1.5432, Accuracy: 30.62%\n",
      "Epoch [53/1000], Batch [10/16], Loss: 1.5178, Accuracy: 30.94%\n",
      "Epoch [54/1000], Batch [10/16], Loss: 1.5245, Accuracy: 30.47%\n",
      "Epoch [55/1000], Batch [10/16], Loss: 1.5125, Accuracy: 32.03%\n",
      "Epoch [56/1000], Batch [10/16], Loss: 1.5305, Accuracy: 31.56%\n",
      "Epoch [57/1000], Batch [10/16], Loss: 1.5029, Accuracy: 34.84%\n",
      "Epoch [58/1000], Batch [10/16], Loss: 1.5385, Accuracy: 29.53%\n",
      "Epoch [59/1000], Batch [10/16], Loss: 1.5172, Accuracy: 31.72%\n",
      "Epoch [60/1000], Batch [10/16], Loss: 1.5183, Accuracy: 33.28%\n",
      "Epoch [61/1000], Batch [10/16], Loss: 1.5184, Accuracy: 33.28%\n",
      "Epoch [62/1000], Batch [10/16], Loss: 1.5313, Accuracy: 32.03%\n",
      "Epoch [63/1000], Batch [10/16], Loss: 1.5080, Accuracy: 34.53%\n",
      "Epoch [64/1000], Batch [10/16], Loss: 1.5092, Accuracy: 32.81%\n",
      "Epoch [65/1000], Batch [10/16], Loss: 1.5133, Accuracy: 32.66%\n",
      "Epoch [66/1000], Batch [10/16], Loss: 1.5070, Accuracy: 33.12%\n",
      "Epoch [67/1000], Batch [10/16], Loss: 1.5045, Accuracy: 33.91%\n",
      "Epoch [68/1000], Batch [10/16], Loss: 1.5158, Accuracy: 33.28%\n",
      "Epoch [69/1000], Batch [10/16], Loss: 1.5158, Accuracy: 32.97%\n",
      "Epoch [70/1000], Batch [10/16], Loss: 1.5029, Accuracy: 34.69%\n",
      "Epoch [71/1000], Batch [10/16], Loss: 1.5032, Accuracy: 33.59%\n",
      "Epoch [72/1000], Batch [10/16], Loss: 1.4970, Accuracy: 33.75%\n",
      "Epoch [73/1000], Batch [10/16], Loss: 1.5169, Accuracy: 33.44%\n",
      "Epoch [74/1000], Batch [10/16], Loss: 1.4996, Accuracy: 34.38%\n",
      "Epoch [75/1000], Batch [10/16], Loss: 1.5080, Accuracy: 33.44%\n",
      "Epoch [76/1000], Batch [10/16], Loss: 1.5083, Accuracy: 32.81%\n",
      "Epoch [77/1000], Batch [10/16], Loss: 1.5080, Accuracy: 32.97%\n",
      "Epoch [78/1000], Batch [10/16], Loss: 1.4991, Accuracy: 34.38%\n",
      "Epoch [79/1000], Batch [10/16], Loss: 1.4975, Accuracy: 34.22%\n",
      "Epoch [80/1000], Batch [10/16], Loss: 1.5083, Accuracy: 32.50%\n",
      "Epoch [81/1000], Batch [10/16], Loss: 1.4970, Accuracy: 33.44%\n",
      "Epoch [82/1000], Batch [10/16], Loss: 1.5011, Accuracy: 33.59%\n",
      "Epoch [83/1000], Batch [10/16], Loss: 1.4928, Accuracy: 33.75%\n",
      "Epoch [84/1000], Batch [10/16], Loss: 1.4937, Accuracy: 35.47%\n",
      "Epoch [85/1000], Batch [10/16], Loss: 1.4840, Accuracy: 34.84%\n",
      "Epoch [86/1000], Batch [10/16], Loss: 1.5035, Accuracy: 34.69%\n",
      "Epoch [87/1000], Batch [10/16], Loss: 1.4873, Accuracy: 35.00%\n",
      "Epoch [88/1000], Batch [10/16], Loss: 1.4906, Accuracy: 33.91%\n",
      "Epoch [89/1000], Batch [10/16], Loss: 1.4802, Accuracy: 36.09%\n",
      "Epoch [90/1000], Batch [10/16], Loss: 1.4963, Accuracy: 35.47%\n",
      "Epoch [91/1000], Batch [10/16], Loss: 1.4997, Accuracy: 36.41%\n",
      "Epoch [92/1000], Batch [10/16], Loss: 1.5158, Accuracy: 32.81%\n",
      "Epoch [93/1000], Batch [10/16], Loss: 1.4987, Accuracy: 35.78%\n",
      "Epoch [94/1000], Batch [10/16], Loss: 1.5059, Accuracy: 32.81%\n",
      "Epoch [95/1000], Batch [10/16], Loss: 1.4869, Accuracy: 35.78%\n",
      "Epoch [96/1000], Batch [10/16], Loss: 1.4864, Accuracy: 34.53%\n",
      "Epoch [97/1000], Batch [10/16], Loss: 1.5031, Accuracy: 34.22%\n",
      "Epoch [98/1000], Batch [10/16], Loss: 1.4773, Accuracy: 38.28%\n",
      "Epoch [99/1000], Batch [10/16], Loss: 1.4976, Accuracy: 36.72%\n",
      "Epoch [100/1000], Batch [10/16], Loss: 1.5000, Accuracy: 33.91%\n",
      "Epoch [101/1000], Batch [10/16], Loss: 1.4958, Accuracy: 34.38%\n",
      "Epoch [102/1000], Batch [10/16], Loss: 1.4775, Accuracy: 37.50%\n",
      "Epoch [103/1000], Batch [10/16], Loss: 1.4789, Accuracy: 37.03%\n",
      "Epoch [104/1000], Batch [10/16], Loss: 1.4831, Accuracy: 36.25%\n",
      "Epoch [105/1000], Batch [10/16], Loss: 1.4817, Accuracy: 37.03%\n",
      "Epoch [106/1000], Batch [10/16], Loss: 1.4829, Accuracy: 35.78%\n",
      "Epoch [107/1000], Batch [10/16], Loss: 1.4793, Accuracy: 36.25%\n",
      "Epoch [108/1000], Batch [10/16], Loss: 1.4728, Accuracy: 36.56%\n",
      "Epoch [109/1000], Batch [10/16], Loss: 1.4840, Accuracy: 35.00%\n",
      "Epoch [110/1000], Batch [10/16], Loss: 1.4795, Accuracy: 36.72%\n",
      "Epoch [111/1000], Batch [10/16], Loss: 1.4930, Accuracy: 34.53%\n",
      "Epoch [112/1000], Batch [10/16], Loss: 1.4847, Accuracy: 36.56%\n",
      "Epoch [113/1000], Batch [10/16], Loss: 1.4755, Accuracy: 34.69%\n",
      "Epoch [114/1000], Batch [10/16], Loss: 1.4830, Accuracy: 35.94%\n",
      "Epoch [115/1000], Batch [10/16], Loss: 1.4767, Accuracy: 37.19%\n",
      "Epoch [116/1000], Batch [10/16], Loss: 1.4788, Accuracy: 36.41%\n",
      "Epoch [117/1000], Batch [10/16], Loss: 1.4816, Accuracy: 35.62%\n",
      "Epoch [118/1000], Batch [10/16], Loss: 1.5153, Accuracy: 32.81%\n",
      "Epoch [119/1000], Batch [10/16], Loss: 1.4930, Accuracy: 35.62%\n",
      "Epoch [120/1000], Batch [10/16], Loss: 1.4811, Accuracy: 35.78%\n",
      "Epoch [121/1000], Batch [10/16], Loss: 1.4741, Accuracy: 37.66%\n",
      "Epoch [122/1000], Batch [10/16], Loss: 1.4885, Accuracy: 35.47%\n",
      "Epoch [123/1000], Batch [10/16], Loss: 1.4755, Accuracy: 35.78%\n",
      "Epoch [124/1000], Batch [10/16], Loss: 1.4845, Accuracy: 35.78%\n",
      "Epoch [125/1000], Batch [10/16], Loss: 1.4759, Accuracy: 36.25%\n",
      "Epoch [126/1000], Batch [10/16], Loss: 1.4682, Accuracy: 37.97%\n",
      "Epoch [127/1000], Batch [10/16], Loss: 1.4824, Accuracy: 36.25%\n",
      "Epoch [128/1000], Batch [10/16], Loss: 1.4842, Accuracy: 35.16%\n",
      "Epoch [129/1000], Batch [10/16], Loss: 1.5048, Accuracy: 33.75%\n",
      "Epoch [130/1000], Batch [10/16], Loss: 1.4754, Accuracy: 36.25%\n",
      "Epoch [131/1000], Batch [10/16], Loss: 1.4853, Accuracy: 35.00%\n",
      "Epoch [132/1000], Batch [10/16], Loss: 1.4756, Accuracy: 36.41%\n",
      "Epoch [133/1000], Batch [10/16], Loss: 1.4689, Accuracy: 36.56%\n",
      "Epoch [134/1000], Batch [10/16], Loss: 1.4989, Accuracy: 34.38%\n",
      "Epoch [135/1000], Batch [10/16], Loss: 1.4461, Accuracy: 38.44%\n",
      "Epoch [136/1000], Batch [10/16], Loss: 1.4820, Accuracy: 36.09%\n",
      "Epoch [137/1000], Batch [10/16], Loss: 1.4726, Accuracy: 36.88%\n",
      "Epoch [138/1000], Batch [10/16], Loss: 1.4796, Accuracy: 36.41%\n",
      "Epoch [139/1000], Batch [10/16], Loss: 1.4688, Accuracy: 37.50%\n",
      "Epoch [140/1000], Batch [10/16], Loss: 1.4841, Accuracy: 35.78%\n",
      "Epoch [141/1000], Batch [10/16], Loss: 1.4829, Accuracy: 34.69%\n",
      "Epoch [142/1000], Batch [10/16], Loss: 1.4674, Accuracy: 37.03%\n",
      "Epoch [143/1000], Batch [10/16], Loss: 1.4572, Accuracy: 37.66%\n",
      "Epoch [144/1000], Batch [10/16], Loss: 1.4807, Accuracy: 37.19%\n",
      "Epoch [145/1000], Batch [10/16], Loss: 1.4656, Accuracy: 35.78%\n",
      "Epoch [146/1000], Batch [10/16], Loss: 1.4767, Accuracy: 35.31%\n",
      "Epoch [147/1000], Batch [10/16], Loss: 1.4759, Accuracy: 37.03%\n",
      "Epoch [148/1000], Batch [10/16], Loss: 1.4786, Accuracy: 36.41%\n",
      "Epoch [149/1000], Batch [10/16], Loss: 1.4701, Accuracy: 36.09%\n",
      "Epoch [150/1000], Batch [10/16], Loss: 1.4578, Accuracy: 36.41%\n",
      "Epoch [151/1000], Batch [10/16], Loss: 1.4763, Accuracy: 35.62%\n",
      "Epoch [152/1000], Batch [10/16], Loss: 1.4612, Accuracy: 37.66%\n",
      "Epoch [153/1000], Batch [10/16], Loss: 1.4643, Accuracy: 35.94%\n",
      "Epoch [154/1000], Batch [10/16], Loss: 1.4670, Accuracy: 36.25%\n",
      "Epoch [155/1000], Batch [10/16], Loss: 1.4937, Accuracy: 34.84%\n",
      "Epoch [156/1000], Batch [10/16], Loss: 1.4798, Accuracy: 34.84%\n",
      "Epoch [157/1000], Batch [10/16], Loss: 1.4820, Accuracy: 36.88%\n",
      "Epoch [158/1000], Batch [10/16], Loss: 1.4796, Accuracy: 34.84%\n",
      "Epoch [159/1000], Batch [10/16], Loss: 1.4900, Accuracy: 33.28%\n",
      "Epoch [160/1000], Batch [10/16], Loss: 1.4761, Accuracy: 34.53%\n",
      "Epoch [161/1000], Batch [10/16], Loss: 1.4693, Accuracy: 37.34%\n",
      "Epoch [162/1000], Batch [10/16], Loss: 1.4755, Accuracy: 36.72%\n",
      "Epoch [163/1000], Batch [10/16], Loss: 1.4793, Accuracy: 37.34%\n",
      "Epoch [164/1000], Batch [10/16], Loss: 1.4526, Accuracy: 37.34%\n",
      "Epoch [165/1000], Batch [10/16], Loss: 1.4548, Accuracy: 37.19%\n",
      "Epoch [166/1000], Batch [10/16], Loss: 1.4663, Accuracy: 36.09%\n",
      "Epoch [167/1000], Batch [10/16], Loss: 1.4614, Accuracy: 36.41%\n",
      "Epoch [168/1000], Batch [10/16], Loss: 1.4834, Accuracy: 34.69%\n",
      "Epoch [169/1000], Batch [10/16], Loss: 1.4593, Accuracy: 35.94%\n",
      "Epoch [170/1000], Batch [10/16], Loss: 1.4616, Accuracy: 35.94%\n",
      "Epoch [171/1000], Batch [10/16], Loss: 1.4722, Accuracy: 35.47%\n",
      "Epoch [172/1000], Batch [10/16], Loss: 1.4659, Accuracy: 34.38%\n",
      "Epoch [173/1000], Batch [10/16], Loss: 1.4750, Accuracy: 36.56%\n",
      "Epoch [174/1000], Batch [10/16], Loss: 1.4607, Accuracy: 38.28%\n",
      "Epoch [175/1000], Batch [10/16], Loss: 1.4733, Accuracy: 35.16%\n",
      "Epoch [176/1000], Batch [10/16], Loss: 1.4736, Accuracy: 35.62%\n",
      "Epoch [177/1000], Batch [10/16], Loss: 1.4576, Accuracy: 38.28%\n",
      "Epoch [178/1000], Batch [10/16], Loss: 1.4901, Accuracy: 33.59%\n",
      "Epoch [179/1000], Batch [10/16], Loss: 1.4573, Accuracy: 36.72%\n",
      "Epoch [180/1000], Batch [10/16], Loss: 1.4731, Accuracy: 35.31%\n",
      "Epoch [181/1000], Batch [10/16], Loss: 1.4648, Accuracy: 36.72%\n",
      "Epoch [182/1000], Batch [10/16], Loss: 1.4794, Accuracy: 34.53%\n",
      "Epoch [183/1000], Batch [10/16], Loss: 1.4712, Accuracy: 35.94%\n",
      "Epoch [184/1000], Batch [10/16], Loss: 1.4416, Accuracy: 36.88%\n",
      "Epoch [185/1000], Batch [10/16], Loss: 1.4657, Accuracy: 36.25%\n",
      "Epoch [186/1000], Batch [10/16], Loss: 1.4656, Accuracy: 36.72%\n",
      "Epoch [187/1000], Batch [10/16], Loss: 1.4667, Accuracy: 37.50%\n",
      "Epoch [188/1000], Batch [10/16], Loss: 1.4568, Accuracy: 36.56%\n",
      "Epoch [189/1000], Batch [10/16], Loss: 1.4804, Accuracy: 34.69%\n",
      "Epoch [190/1000], Batch [10/16], Loss: 1.4665, Accuracy: 37.03%\n",
      "Epoch [191/1000], Batch [10/16], Loss: 1.4674, Accuracy: 36.41%\n",
      "Epoch [192/1000], Batch [10/16], Loss: 1.4537, Accuracy: 36.72%\n",
      "Epoch [193/1000], Batch [10/16], Loss: 1.4593, Accuracy: 36.56%\n",
      "Epoch [194/1000], Batch [10/16], Loss: 1.4581, Accuracy: 35.94%\n",
      "Epoch [195/1000], Batch [10/16], Loss: 1.4571, Accuracy: 36.88%\n",
      "Epoch [196/1000], Batch [10/16], Loss: 1.4797, Accuracy: 34.84%\n",
      "Epoch [197/1000], Batch [10/16], Loss: 1.4881, Accuracy: 35.78%\n",
      "Epoch [198/1000], Batch [10/16], Loss: 1.4563, Accuracy: 37.66%\n",
      "Epoch [199/1000], Batch [10/16], Loss: 1.4739, Accuracy: 35.16%\n",
      "Epoch [200/1000], Batch [10/16], Loss: 1.4642, Accuracy: 35.16%\n",
      "Epoch [201/1000], Batch [10/16], Loss: 1.4754, Accuracy: 36.56%\n",
      "Epoch [202/1000], Batch [10/16], Loss: 1.4725, Accuracy: 35.78%\n",
      "Epoch [203/1000], Batch [10/16], Loss: 1.4580, Accuracy: 36.56%\n",
      "Epoch [204/1000], Batch [10/16], Loss: 1.4711, Accuracy: 35.78%\n",
      "Epoch [205/1000], Batch [10/16], Loss: 1.4656, Accuracy: 36.25%\n",
      "Epoch [206/1000], Batch [10/16], Loss: 1.4804, Accuracy: 35.31%\n",
      "Epoch [207/1000], Batch [10/16], Loss: 1.4472, Accuracy: 39.22%\n",
      "Epoch [208/1000], Batch [10/16], Loss: 1.4616, Accuracy: 36.09%\n",
      "Epoch [209/1000], Batch [10/16], Loss: 1.4385, Accuracy: 37.19%\n",
      "Epoch [210/1000], Batch [10/16], Loss: 1.4402, Accuracy: 37.03%\n",
      "Epoch [211/1000], Batch [10/16], Loss: 1.4677, Accuracy: 36.41%\n",
      "Epoch [212/1000], Batch [10/16], Loss: 1.4724, Accuracy: 35.16%\n",
      "Epoch [213/1000], Batch [10/16], Loss: 1.4587, Accuracy: 36.56%\n",
      "Epoch [214/1000], Batch [10/16], Loss: 1.4833, Accuracy: 35.47%\n",
      "Epoch [215/1000], Batch [10/16], Loss: 1.4621, Accuracy: 36.25%\n",
      "Epoch [216/1000], Batch [10/16], Loss: 1.4632, Accuracy: 36.09%\n",
      "Epoch [217/1000], Batch [10/16], Loss: 1.4766, Accuracy: 35.62%\n",
      "Epoch [218/1000], Batch [10/16], Loss: 1.4645, Accuracy: 36.09%\n",
      "Epoch [219/1000], Batch [10/16], Loss: 1.4630, Accuracy: 37.19%\n",
      "Epoch [220/1000], Batch [10/16], Loss: 1.4596, Accuracy: 36.09%\n",
      "Epoch [221/1000], Batch [10/16], Loss: 1.4459, Accuracy: 37.03%\n",
      "Epoch [222/1000], Batch [10/16], Loss: 1.4688, Accuracy: 35.16%\n",
      "Epoch [223/1000], Batch [10/16], Loss: 1.4659, Accuracy: 34.69%\n",
      "Epoch [224/1000], Batch [10/16], Loss: 1.4523, Accuracy: 35.94%\n",
      "Epoch [225/1000], Batch [10/16], Loss: 1.4734, Accuracy: 34.38%\n",
      "Epoch [226/1000], Batch [10/16], Loss: 1.4478, Accuracy: 37.34%\n",
      "Epoch [227/1000], Batch [10/16], Loss: 1.4568, Accuracy: 34.53%\n",
      "Epoch [228/1000], Batch [10/16], Loss: 1.4544, Accuracy: 36.56%\n",
      "Epoch [229/1000], Batch [10/16], Loss: 1.4666, Accuracy: 36.09%\n",
      "Epoch [230/1000], Batch [10/16], Loss: 1.4559, Accuracy: 37.34%\n",
      "Epoch [231/1000], Batch [10/16], Loss: 1.4498, Accuracy: 36.25%\n",
      "Epoch [232/1000], Batch [10/16], Loss: 1.4819, Accuracy: 35.62%\n",
      "Epoch [233/1000], Batch [10/16], Loss: 1.4546, Accuracy: 37.97%\n",
      "Epoch [234/1000], Batch [10/16], Loss: 1.4596, Accuracy: 36.72%\n",
      "Epoch [235/1000], Batch [10/16], Loss: 1.4706, Accuracy: 34.84%\n",
      "Epoch [236/1000], Batch [10/16], Loss: 1.4817, Accuracy: 34.38%\n",
      "Epoch [237/1000], Batch [10/16], Loss: 1.4668, Accuracy: 37.19%\n",
      "Epoch [238/1000], Batch [10/16], Loss: 1.4712, Accuracy: 35.47%\n",
      "Epoch [239/1000], Batch [10/16], Loss: 1.4663, Accuracy: 36.09%\n",
      "Epoch [240/1000], Batch [10/16], Loss: 1.4525, Accuracy: 36.72%\n",
      "Epoch [241/1000], Batch [10/16], Loss: 1.4785, Accuracy: 33.75%\n",
      "Epoch [242/1000], Batch [10/16], Loss: 1.4607, Accuracy: 36.41%\n",
      "Epoch [243/1000], Batch [10/16], Loss: 1.4797, Accuracy: 34.38%\n",
      "Epoch [244/1000], Batch [10/16], Loss: 1.4487, Accuracy: 36.88%\n",
      "Epoch [245/1000], Batch [10/16], Loss: 1.4537, Accuracy: 36.56%\n",
      "Epoch [246/1000], Batch [10/16], Loss: 1.4701, Accuracy: 34.69%\n",
      "Epoch [247/1000], Batch [10/16], Loss: 1.4553, Accuracy: 36.56%\n",
      "Epoch [248/1000], Batch [10/16], Loss: 1.4555, Accuracy: 35.31%\n",
      "Epoch [249/1000], Batch [10/16], Loss: 1.4686, Accuracy: 35.16%\n",
      "Epoch [250/1000], Batch [10/16], Loss: 1.4617, Accuracy: 36.72%\n",
      "Epoch [251/1000], Batch [10/16], Loss: 1.4773, Accuracy: 37.03%\n",
      "Epoch [252/1000], Batch [10/16], Loss: 1.4611, Accuracy: 37.19%\n",
      "Epoch [253/1000], Batch [10/16], Loss: 1.4701, Accuracy: 35.62%\n",
      "Epoch [254/1000], Batch [10/16], Loss: 1.4604, Accuracy: 35.62%\n",
      "Epoch [255/1000], Batch [10/16], Loss: 1.4547, Accuracy: 35.62%\n",
      "Epoch [256/1000], Batch [10/16], Loss: 1.4814, Accuracy: 36.41%\n",
      "Epoch [257/1000], Batch [10/16], Loss: 1.4537, Accuracy: 35.16%\n",
      "Epoch [258/1000], Batch [10/16], Loss: 1.4768, Accuracy: 35.94%\n",
      "Epoch [259/1000], Batch [10/16], Loss: 1.4625, Accuracy: 35.31%\n",
      "Epoch [260/1000], Batch [10/16], Loss: 1.4536, Accuracy: 36.09%\n",
      "Epoch [261/1000], Batch [10/16], Loss: 1.4518, Accuracy: 37.97%\n",
      "Epoch [262/1000], Batch [10/16], Loss: 1.4497, Accuracy: 35.31%\n",
      "Epoch [263/1000], Batch [10/16], Loss: 1.4602, Accuracy: 34.38%\n",
      "Epoch [264/1000], Batch [10/16], Loss: 1.4579, Accuracy: 36.09%\n",
      "Epoch [265/1000], Batch [10/16], Loss: 1.4530, Accuracy: 36.25%\n",
      "Epoch [266/1000], Batch [10/16], Loss: 1.4677, Accuracy: 36.56%\n",
      "Epoch [267/1000], Batch [10/16], Loss: 1.4622, Accuracy: 35.16%\n",
      "Epoch [268/1000], Batch [10/16], Loss: 1.4539, Accuracy: 35.31%\n",
      "Epoch [269/1000], Batch [10/16], Loss: 1.4477, Accuracy: 37.03%\n",
      "Epoch [270/1000], Batch [10/16], Loss: 1.4783, Accuracy: 34.38%\n",
      "Epoch [271/1000], Batch [10/16], Loss: 1.4656, Accuracy: 35.78%\n",
      "Epoch [272/1000], Batch [10/16], Loss: 1.4396, Accuracy: 36.72%\n",
      "Epoch [273/1000], Batch [10/16], Loss: 1.4594, Accuracy: 35.16%\n",
      "Epoch [274/1000], Batch [10/16], Loss: 1.4637, Accuracy: 35.94%\n",
      "Epoch [275/1000], Batch [10/16], Loss: 1.4770, Accuracy: 35.31%\n",
      "Epoch [276/1000], Batch [10/16], Loss: 1.4686, Accuracy: 35.78%\n",
      "Epoch [277/1000], Batch [10/16], Loss: 1.4617, Accuracy: 35.47%\n",
      "Epoch [278/1000], Batch [10/16], Loss: 1.4585, Accuracy: 36.25%\n",
      "Epoch [279/1000], Batch [10/16], Loss: 1.4631, Accuracy: 35.31%\n",
      "Epoch [280/1000], Batch [10/16], Loss: 1.4581, Accuracy: 35.47%\n",
      "Epoch [281/1000], Batch [10/16], Loss: 1.4752, Accuracy: 32.81%\n",
      "Epoch [282/1000], Batch [10/16], Loss: 1.4540, Accuracy: 36.25%\n",
      "Epoch [283/1000], Batch [10/16], Loss: 1.4597, Accuracy: 36.72%\n",
      "Epoch [284/1000], Batch [10/16], Loss: 1.4557, Accuracy: 35.00%\n",
      "Epoch [285/1000], Batch [10/16], Loss: 1.4809, Accuracy: 33.12%\n",
      "Epoch [286/1000], Batch [10/16], Loss: 1.4447, Accuracy: 35.94%\n",
      "Epoch [287/1000], Batch [10/16], Loss: 1.4368, Accuracy: 36.88%\n",
      "Epoch [288/1000], Batch [10/16], Loss: 1.4706, Accuracy: 35.00%\n",
      "Epoch [289/1000], Batch [10/16], Loss: 1.4488, Accuracy: 37.97%\n",
      "Epoch [290/1000], Batch [10/16], Loss: 1.4625, Accuracy: 34.53%\n",
      "Epoch [291/1000], Batch [10/16], Loss: 1.4369, Accuracy: 36.72%\n",
      "Epoch [292/1000], Batch [10/16], Loss: 1.4302, Accuracy: 37.19%\n",
      "Epoch [293/1000], Batch [10/16], Loss: 1.4317, Accuracy: 35.62%\n",
      "Epoch [294/1000], Batch [10/16], Loss: 1.4481, Accuracy: 35.31%\n",
      "Epoch [295/1000], Batch [10/16], Loss: 1.4505, Accuracy: 36.41%\n",
      "Epoch [296/1000], Batch [10/16], Loss: 1.4644, Accuracy: 35.31%\n",
      "Epoch [297/1000], Batch [10/16], Loss: 1.4472, Accuracy: 37.19%\n",
      "Epoch [298/1000], Batch [10/16], Loss: 1.4605, Accuracy: 35.94%\n",
      "Epoch [299/1000], Batch [10/16], Loss: 1.4638, Accuracy: 34.69%\n",
      "Epoch [300/1000], Batch [10/16], Loss: 1.4673, Accuracy: 35.78%\n",
      "Epoch [301/1000], Batch [10/16], Loss: 1.4703, Accuracy: 34.69%\n",
      "Epoch [302/1000], Batch [10/16], Loss: 1.4548, Accuracy: 36.41%\n",
      "Epoch [303/1000], Batch [10/16], Loss: 1.4524, Accuracy: 35.62%\n",
      "Epoch [304/1000], Batch [10/16], Loss: 1.4536, Accuracy: 34.69%\n",
      "Epoch [305/1000], Batch [10/16], Loss: 1.4188, Accuracy: 38.28%\n",
      "Epoch [306/1000], Batch [10/16], Loss: 1.4326, Accuracy: 37.66%\n",
      "Epoch [307/1000], Batch [10/16], Loss: 1.4736, Accuracy: 34.84%\n",
      "Epoch [308/1000], Batch [10/16], Loss: 1.4648, Accuracy: 34.53%\n",
      "Epoch [309/1000], Batch [10/16], Loss: 1.4654, Accuracy: 36.09%\n",
      "Epoch [310/1000], Batch [10/16], Loss: 1.4631, Accuracy: 34.84%\n",
      "Epoch [311/1000], Batch [10/16], Loss: 1.4348, Accuracy: 36.25%\n",
      "Epoch [312/1000], Batch [10/16], Loss: 1.4619, Accuracy: 35.31%\n",
      "Epoch [313/1000], Batch [10/16], Loss: 1.4524, Accuracy: 36.25%\n",
      "Epoch [314/1000], Batch [10/16], Loss: 1.4353, Accuracy: 36.88%\n",
      "Epoch [315/1000], Batch [10/16], Loss: 1.4520, Accuracy: 35.16%\n",
      "Epoch [316/1000], Batch [10/16], Loss: 1.4688, Accuracy: 34.38%\n",
      "Epoch [317/1000], Batch [10/16], Loss: 1.4504, Accuracy: 35.78%\n",
      "Epoch [318/1000], Batch [10/16], Loss: 1.4387, Accuracy: 37.19%\n",
      "Epoch [319/1000], Batch [10/16], Loss: 1.4536, Accuracy: 36.09%\n",
      "Epoch [320/1000], Batch [10/16], Loss: 1.4715, Accuracy: 34.38%\n",
      "Epoch [321/1000], Batch [10/16], Loss: 1.4603, Accuracy: 35.62%\n",
      "Epoch [322/1000], Batch [10/16], Loss: 1.4476, Accuracy: 35.62%\n",
      "Epoch [323/1000], Batch [10/16], Loss: 1.4676, Accuracy: 34.84%\n",
      "Epoch [324/1000], Batch [10/16], Loss: 1.4458, Accuracy: 36.09%\n",
      "Epoch [325/1000], Batch [10/16], Loss: 1.4681, Accuracy: 35.00%\n",
      "Epoch [326/1000], Batch [10/16], Loss: 1.4537, Accuracy: 35.31%\n",
      "Epoch [327/1000], Batch [10/16], Loss: 1.4361, Accuracy: 37.81%\n",
      "Epoch [328/1000], Batch [10/16], Loss: 1.4531, Accuracy: 35.47%\n",
      "Epoch [329/1000], Batch [10/16], Loss: 1.4666, Accuracy: 35.00%\n",
      "Epoch [330/1000], Batch [10/16], Loss: 1.4527, Accuracy: 36.09%\n",
      "Epoch [331/1000], Batch [10/16], Loss: 1.4486, Accuracy: 36.88%\n",
      "Epoch [332/1000], Batch [10/16], Loss: 1.4676, Accuracy: 32.97%\n",
      "Epoch [333/1000], Batch [10/16], Loss: 1.4843, Accuracy: 33.75%\n",
      "Epoch [334/1000], Batch [10/16], Loss: 1.4398, Accuracy: 35.62%\n",
      "Epoch [335/1000], Batch [10/16], Loss: 1.4655, Accuracy: 34.22%\n",
      "Epoch [336/1000], Batch [10/16], Loss: 1.4739, Accuracy: 33.75%\n",
      "Epoch [337/1000], Batch [10/16], Loss: 1.4460, Accuracy: 35.78%\n",
      "Epoch [338/1000], Batch [10/16], Loss: 1.4578, Accuracy: 35.47%\n",
      "Epoch [339/1000], Batch [10/16], Loss: 1.4444, Accuracy: 36.56%\n",
      "Epoch [340/1000], Batch [10/16], Loss: 1.4534, Accuracy: 35.78%\n",
      "Epoch [341/1000], Batch [10/16], Loss: 1.4362, Accuracy: 37.03%\n",
      "Epoch [342/1000], Batch [10/16], Loss: 1.4570, Accuracy: 35.16%\n",
      "Epoch [343/1000], Batch [10/16], Loss: 1.4783, Accuracy: 33.59%\n",
      "Epoch [344/1000], Batch [10/16], Loss: 1.4517, Accuracy: 35.47%\n",
      "Epoch [345/1000], Batch [10/16], Loss: 1.4483, Accuracy: 36.72%\n",
      "Epoch [346/1000], Batch [10/16], Loss: 1.4434, Accuracy: 37.03%\n",
      "Epoch [347/1000], Batch [10/16], Loss: 1.4544, Accuracy: 35.31%\n",
      "Epoch [348/1000], Batch [10/16], Loss: 1.4283, Accuracy: 38.44%\n",
      "Epoch [349/1000], Batch [10/16], Loss: 1.4545, Accuracy: 37.19%\n",
      "Epoch [350/1000], Batch [10/16], Loss: 1.4611, Accuracy: 36.09%\n",
      "Epoch [351/1000], Batch [10/16], Loss: 1.4598, Accuracy: 35.78%\n",
      "Epoch [352/1000], Batch [10/16], Loss: 1.4495, Accuracy: 35.31%\n",
      "Epoch [353/1000], Batch [10/16], Loss: 1.4501, Accuracy: 36.56%\n",
      "Epoch [354/1000], Batch [10/16], Loss: 1.4347, Accuracy: 39.06%\n",
      "Epoch [355/1000], Batch [10/16], Loss: 1.4531, Accuracy: 35.16%\n",
      "Epoch [356/1000], Batch [10/16], Loss: 1.4804, Accuracy: 33.44%\n",
      "Epoch [357/1000], Batch [10/16], Loss: 1.4565, Accuracy: 34.69%\n",
      "Epoch [358/1000], Batch [10/16], Loss: 1.4582, Accuracy: 35.00%\n",
      "Epoch [359/1000], Batch [10/16], Loss: 1.4548, Accuracy: 35.62%\n",
      "Epoch [360/1000], Batch [10/16], Loss: 1.4389, Accuracy: 37.34%\n",
      "Epoch [361/1000], Batch [10/16], Loss: 1.4480, Accuracy: 36.25%\n",
      "Epoch [362/1000], Batch [10/16], Loss: 1.4320, Accuracy: 38.75%\n",
      "Epoch [363/1000], Batch [10/16], Loss: 1.4918, Accuracy: 31.88%\n",
      "Epoch [364/1000], Batch [10/16], Loss: 1.4490, Accuracy: 36.09%\n",
      "Epoch [365/1000], Batch [10/16], Loss: 1.4497, Accuracy: 36.09%\n",
      "Epoch [366/1000], Batch [10/16], Loss: 1.4549, Accuracy: 35.31%\n",
      "Epoch [367/1000], Batch [10/16], Loss: 1.4689, Accuracy: 35.00%\n",
      "Epoch [368/1000], Batch [10/16], Loss: 1.4476, Accuracy: 34.22%\n",
      "Epoch [369/1000], Batch [10/16], Loss: 1.4447, Accuracy: 35.62%\n",
      "Epoch [370/1000], Batch [10/16], Loss: 1.4459, Accuracy: 35.94%\n",
      "Epoch [371/1000], Batch [10/16], Loss: 1.4537, Accuracy: 35.47%\n",
      "Epoch [372/1000], Batch [10/16], Loss: 1.4369, Accuracy: 36.56%\n",
      "Epoch [373/1000], Batch [10/16], Loss: 1.4320, Accuracy: 37.50%\n",
      "Epoch [374/1000], Batch [10/16], Loss: 1.4336, Accuracy: 37.03%\n",
      "Epoch [375/1000], Batch [10/16], Loss: 1.4350, Accuracy: 37.50%\n",
      "Epoch [376/1000], Batch [10/16], Loss: 1.4437, Accuracy: 36.41%\n",
      "Epoch [377/1000], Batch [10/16], Loss: 1.4518, Accuracy: 35.31%\n",
      "Epoch [378/1000], Batch [10/16], Loss: 1.4552, Accuracy: 36.09%\n",
      "Epoch [379/1000], Batch [10/16], Loss: 1.4674, Accuracy: 34.53%\n",
      "Epoch [380/1000], Batch [10/16], Loss: 1.4472, Accuracy: 36.25%\n",
      "Epoch [381/1000], Batch [10/16], Loss: 1.4644, Accuracy: 34.22%\n",
      "Epoch [382/1000], Batch [10/16], Loss: 1.4450, Accuracy: 36.88%\n",
      "Epoch [383/1000], Batch [10/16], Loss: 1.4467, Accuracy: 36.56%\n",
      "Epoch [384/1000], Batch [10/16], Loss: 1.4351, Accuracy: 36.56%\n",
      "Epoch [385/1000], Batch [10/16], Loss: 1.4552, Accuracy: 36.72%\n",
      "Epoch [386/1000], Batch [10/16], Loss: 1.4283, Accuracy: 37.19%\n",
      "Epoch [387/1000], Batch [10/16], Loss: 1.4551, Accuracy: 36.88%\n",
      "Epoch [388/1000], Batch [10/16], Loss: 1.4476, Accuracy: 37.03%\n",
      "Epoch [389/1000], Batch [10/16], Loss: 1.4669, Accuracy: 35.62%\n",
      "Epoch [390/1000], Batch [10/16], Loss: 1.4476, Accuracy: 34.53%\n",
      "Epoch [391/1000], Batch [10/16], Loss: 1.4772, Accuracy: 33.59%\n",
      "Epoch [392/1000], Batch [10/16], Loss: 1.4667, Accuracy: 34.22%\n",
      "Epoch [393/1000], Batch [10/16], Loss: 1.4565, Accuracy: 35.94%\n",
      "Epoch [394/1000], Batch [10/16], Loss: 1.4667, Accuracy: 34.69%\n",
      "Epoch [395/1000], Batch [10/16], Loss: 1.4402, Accuracy: 36.41%\n",
      "Epoch [396/1000], Batch [10/16], Loss: 1.4601, Accuracy: 33.91%\n",
      "Epoch [397/1000], Batch [10/16], Loss: 1.4541, Accuracy: 35.16%\n",
      "Epoch [398/1000], Batch [10/16], Loss: 1.4474, Accuracy: 37.03%\n",
      "Epoch [399/1000], Batch [10/16], Loss: 1.4551, Accuracy: 35.94%\n",
      "Epoch [400/1000], Batch [10/16], Loss: 1.4441, Accuracy: 36.41%\n",
      "Epoch [401/1000], Batch [10/16], Loss: 1.4633, Accuracy: 34.69%\n",
      "Epoch [402/1000], Batch [10/16], Loss: 1.4741, Accuracy: 33.59%\n",
      "Epoch [403/1000], Batch [10/16], Loss: 1.4542, Accuracy: 35.31%\n",
      "Epoch [404/1000], Batch [10/16], Loss: 1.4685, Accuracy: 34.22%\n",
      "Epoch [405/1000], Batch [10/16], Loss: 1.4542, Accuracy: 36.09%\n",
      "Epoch [406/1000], Batch [10/16], Loss: 1.4498, Accuracy: 36.09%\n",
      "Epoch [407/1000], Batch [10/16], Loss: 1.4451, Accuracy: 35.94%\n",
      "Epoch [408/1000], Batch [10/16], Loss: 1.4563, Accuracy: 35.78%\n",
      "Epoch [409/1000], Batch [10/16], Loss: 1.4317, Accuracy: 37.50%\n",
      "Epoch [410/1000], Batch [10/16], Loss: 1.4548, Accuracy: 35.47%\n",
      "Epoch [411/1000], Batch [10/16], Loss: 1.4448, Accuracy: 36.72%\n",
      "Epoch [412/1000], Batch [10/16], Loss: 1.4420, Accuracy: 36.88%\n",
      "Epoch [413/1000], Batch [10/16], Loss: 1.4594, Accuracy: 35.16%\n",
      "Epoch [414/1000], Batch [10/16], Loss: 1.4478, Accuracy: 37.34%\n",
      "Epoch [415/1000], Batch [10/16], Loss: 1.4468, Accuracy: 36.41%\n",
      "Epoch [416/1000], Batch [10/16], Loss: 1.4530, Accuracy: 35.31%\n",
      "Epoch [417/1000], Batch [10/16], Loss: 1.4155, Accuracy: 37.81%\n",
      "Epoch [418/1000], Batch [10/16], Loss: 1.4198, Accuracy: 38.12%\n",
      "Epoch [419/1000], Batch [10/16], Loss: 1.4578, Accuracy: 36.72%\n",
      "Epoch [420/1000], Batch [10/16], Loss: 1.4691, Accuracy: 33.91%\n",
      "Epoch [421/1000], Batch [10/16], Loss: 1.4274, Accuracy: 38.75%\n",
      "Epoch [422/1000], Batch [10/16], Loss: 1.4566, Accuracy: 34.69%\n",
      "Epoch [423/1000], Batch [10/16], Loss: 1.4650, Accuracy: 36.25%\n",
      "Epoch [424/1000], Batch [10/16], Loss: 1.4266, Accuracy: 37.50%\n",
      "Epoch [425/1000], Batch [10/16], Loss: 1.4521, Accuracy: 35.00%\n",
      "Epoch [426/1000], Batch [10/16], Loss: 1.4425, Accuracy: 37.50%\n",
      "Epoch [427/1000], Batch [10/16], Loss: 1.4536, Accuracy: 37.03%\n",
      "Epoch [428/1000], Batch [10/16], Loss: 1.4448, Accuracy: 37.34%\n",
      "Epoch [429/1000], Batch [10/16], Loss: 1.4471, Accuracy: 36.41%\n",
      "Epoch [430/1000], Batch [10/16], Loss: 1.4537, Accuracy: 36.09%\n",
      "Epoch [431/1000], Batch [10/16], Loss: 1.4603, Accuracy: 36.72%\n",
      "Epoch [432/1000], Batch [10/16], Loss: 1.4480, Accuracy: 35.78%\n",
      "Epoch [433/1000], Batch [10/16], Loss: 1.4461, Accuracy: 36.41%\n",
      "Epoch [434/1000], Batch [10/16], Loss: 1.4467, Accuracy: 35.31%\n",
      "Epoch [435/1000], Batch [10/16], Loss: 1.4525, Accuracy: 38.59%\n",
      "Epoch [436/1000], Batch [10/16], Loss: 1.4540, Accuracy: 35.94%\n",
      "Epoch [437/1000], Batch [10/16], Loss: 1.4260, Accuracy: 40.31%\n",
      "Epoch [438/1000], Batch [10/16], Loss: 1.4634, Accuracy: 35.62%\n",
      "Epoch [439/1000], Batch [10/16], Loss: 1.4656, Accuracy: 35.78%\n",
      "Epoch [440/1000], Batch [10/16], Loss: 1.4482, Accuracy: 35.31%\n",
      "Epoch [441/1000], Batch [10/16], Loss: 1.4594, Accuracy: 35.47%\n",
      "Epoch [442/1000], Batch [10/16], Loss: 1.4501, Accuracy: 35.16%\n",
      "Epoch [443/1000], Batch [10/16], Loss: 1.4368, Accuracy: 36.88%\n",
      "Epoch [444/1000], Batch [10/16], Loss: 1.4211, Accuracy: 38.44%\n",
      "Epoch [445/1000], Batch [10/16], Loss: 1.4533, Accuracy: 36.72%\n",
      "Epoch [446/1000], Batch [10/16], Loss: 1.4359, Accuracy: 38.91%\n",
      "Epoch [447/1000], Batch [10/16], Loss: 1.4385, Accuracy: 37.50%\n",
      "Epoch [448/1000], Batch [10/16], Loss: 1.4343, Accuracy: 37.66%\n",
      "Epoch [449/1000], Batch [10/16], Loss: 1.4499, Accuracy: 36.88%\n",
      "Epoch [450/1000], Batch [10/16], Loss: 1.4360, Accuracy: 38.12%\n",
      "Epoch [451/1000], Batch [10/16], Loss: 1.4622, Accuracy: 35.78%\n",
      "Epoch [452/1000], Batch [10/16], Loss: 1.4398, Accuracy: 37.19%\n",
      "Epoch [453/1000], Batch [10/16], Loss: 1.4576, Accuracy: 37.81%\n",
      "Epoch [454/1000], Batch [10/16], Loss: 1.4579, Accuracy: 35.62%\n",
      "Epoch [455/1000], Batch [10/16], Loss: 1.4375, Accuracy: 37.34%\n",
      "Epoch [456/1000], Batch [10/16], Loss: 1.4538, Accuracy: 36.41%\n",
      "Epoch [457/1000], Batch [10/16], Loss: 1.4594, Accuracy: 36.25%\n",
      "Epoch [458/1000], Batch [10/16], Loss: 1.4473, Accuracy: 39.22%\n",
      "Epoch [459/1000], Batch [10/16], Loss: 1.4469, Accuracy: 36.56%\n",
      "Epoch [460/1000], Batch [10/16], Loss: 1.4499, Accuracy: 37.34%\n",
      "Epoch [461/1000], Batch [10/16], Loss: 1.4523, Accuracy: 37.66%\n",
      "Epoch [462/1000], Batch [10/16], Loss: 1.4305, Accuracy: 37.81%\n",
      "Epoch [463/1000], Batch [10/16], Loss: 1.4210, Accuracy: 37.97%\n",
      "Epoch [464/1000], Batch [10/16], Loss: 1.4312, Accuracy: 38.28%\n",
      "Epoch [465/1000], Batch [10/16], Loss: 1.4590, Accuracy: 37.03%\n",
      "Epoch [466/1000], Batch [10/16], Loss: 1.4267, Accuracy: 38.75%\n",
      "Epoch [467/1000], Batch [10/16], Loss: 1.4365, Accuracy: 38.28%\n",
      "Epoch [468/1000], Batch [10/16], Loss: 1.4375, Accuracy: 39.38%\n",
      "Epoch [469/1000], Batch [10/16], Loss: 1.4443, Accuracy: 37.50%\n",
      "Epoch [470/1000], Batch [10/16], Loss: 1.4371, Accuracy: 37.50%\n",
      "Epoch [471/1000], Batch [10/16], Loss: 1.4588, Accuracy: 36.56%\n",
      "Epoch [472/1000], Batch [10/16], Loss: 1.4273, Accuracy: 39.53%\n",
      "Epoch [473/1000], Batch [10/16], Loss: 1.4452, Accuracy: 37.03%\n",
      "Epoch [474/1000], Batch [10/16], Loss: 1.4177, Accuracy: 38.28%\n",
      "Epoch [475/1000], Batch [10/16], Loss: 1.4515, Accuracy: 39.06%\n",
      "Epoch [476/1000], Batch [10/16], Loss: 1.4493, Accuracy: 37.50%\n",
      "Epoch [477/1000], Batch [10/16], Loss: 1.4702, Accuracy: 36.72%\n",
      "Epoch [478/1000], Batch [10/16], Loss: 1.4634, Accuracy: 34.69%\n",
      "Epoch [479/1000], Batch [10/16], Loss: 1.4408, Accuracy: 37.81%\n",
      "Epoch [480/1000], Batch [10/16], Loss: 1.4478, Accuracy: 38.59%\n",
      "Epoch [481/1000], Batch [10/16], Loss: 1.4577, Accuracy: 36.25%\n",
      "Epoch [482/1000], Batch [10/16], Loss: 1.4388, Accuracy: 37.66%\n",
      "Epoch [483/1000], Batch [10/16], Loss: 1.4304, Accuracy: 39.38%\n",
      "Epoch [484/1000], Batch [10/16], Loss: 1.4306, Accuracy: 38.59%\n",
      "Epoch [485/1000], Batch [10/16], Loss: 1.4340, Accuracy: 38.91%\n",
      "Epoch [486/1000], Batch [10/16], Loss: 1.4537, Accuracy: 35.47%\n",
      "Epoch [487/1000], Batch [10/16], Loss: 1.4620, Accuracy: 34.38%\n",
      "Epoch [488/1000], Batch [10/16], Loss: 1.4287, Accuracy: 37.34%\n",
      "Epoch [489/1000], Batch [10/16], Loss: 1.4610, Accuracy: 35.31%\n",
      "Epoch [490/1000], Batch [10/16], Loss: 1.4362, Accuracy: 38.44%\n",
      "Epoch [491/1000], Batch [10/16], Loss: 1.4309, Accuracy: 38.75%\n",
      "Epoch [492/1000], Batch [10/16], Loss: 1.4448, Accuracy: 37.97%\n",
      "Epoch [493/1000], Batch [10/16], Loss: 1.4407, Accuracy: 37.66%\n",
      "Epoch [494/1000], Batch [10/16], Loss: 1.4400, Accuracy: 37.50%\n",
      "Epoch [495/1000], Batch [10/16], Loss: 1.4370, Accuracy: 38.28%\n",
      "Epoch [496/1000], Batch [10/16], Loss: 1.4701, Accuracy: 34.06%\n",
      "Epoch [497/1000], Batch [10/16], Loss: 1.4557, Accuracy: 36.72%\n",
      "Epoch [498/1000], Batch [10/16], Loss: 1.4360, Accuracy: 37.34%\n",
      "Epoch [499/1000], Batch [10/16], Loss: 1.4457, Accuracy: 37.66%\n",
      "Epoch [500/1000], Batch [10/16], Loss: 1.4461, Accuracy: 36.09%\n",
      "Epoch [501/1000], Batch [10/16], Loss: 1.4261, Accuracy: 37.34%\n",
      "Epoch [502/1000], Batch [10/16], Loss: 1.4422, Accuracy: 36.41%\n",
      "Epoch [503/1000], Batch [10/16], Loss: 1.4264, Accuracy: 38.75%\n",
      "Epoch [504/1000], Batch [10/16], Loss: 1.4334, Accuracy: 38.59%\n",
      "Epoch [505/1000], Batch [10/16], Loss: 1.4270, Accuracy: 39.06%\n",
      "Epoch [506/1000], Batch [10/16], Loss: 1.4362, Accuracy: 37.34%\n",
      "Epoch [507/1000], Batch [10/16], Loss: 1.4397, Accuracy: 37.81%\n",
      "Epoch [508/1000], Batch [10/16], Loss: 1.4287, Accuracy: 37.97%\n",
      "Epoch [509/1000], Batch [10/16], Loss: 1.4493, Accuracy: 35.31%\n",
      "Epoch [510/1000], Batch [10/16], Loss: 1.4533, Accuracy: 37.03%\n",
      "Epoch [511/1000], Batch [10/16], Loss: 1.4548, Accuracy: 36.25%\n",
      "Epoch [512/1000], Batch [10/16], Loss: 1.4359, Accuracy: 36.41%\n",
      "Epoch [513/1000], Batch [10/16], Loss: 1.4342, Accuracy: 37.97%\n",
      "Epoch [514/1000], Batch [10/16], Loss: 1.4422, Accuracy: 36.88%\n",
      "Epoch [515/1000], Batch [10/16], Loss: 1.4267, Accuracy: 37.34%\n",
      "Epoch [516/1000], Batch [10/16], Loss: 1.4410, Accuracy: 35.78%\n",
      "Epoch [517/1000], Batch [10/16], Loss: 1.4435, Accuracy: 36.72%\n",
      "Epoch [518/1000], Batch [10/16], Loss: 1.4321, Accuracy: 36.88%\n",
      "Epoch [519/1000], Batch [10/16], Loss: 1.4417, Accuracy: 35.62%\n",
      "Epoch [520/1000], Batch [10/16], Loss: 1.4276, Accuracy: 38.75%\n",
      "Epoch [521/1000], Batch [10/16], Loss: 1.4282, Accuracy: 36.88%\n",
      "Epoch [522/1000], Batch [10/16], Loss: 1.4237, Accuracy: 38.91%\n",
      "Epoch [523/1000], Batch [10/16], Loss: 1.4573, Accuracy: 35.94%\n",
      "Epoch [524/1000], Batch [10/16], Loss: 1.4382, Accuracy: 36.72%\n",
      "Epoch [525/1000], Batch [10/16], Loss: 1.4335, Accuracy: 38.59%\n",
      "Epoch [526/1000], Batch [10/16], Loss: 1.4534, Accuracy: 35.16%\n",
      "Epoch [527/1000], Batch [10/16], Loss: 1.4209, Accuracy: 37.81%\n",
      "Epoch [528/1000], Batch [10/16], Loss: 1.4347, Accuracy: 37.34%\n",
      "Epoch [529/1000], Batch [10/16], Loss: 1.4313, Accuracy: 37.66%\n",
      "Epoch [530/1000], Batch [10/16], Loss: 1.4360, Accuracy: 36.72%\n",
      "Epoch [531/1000], Batch [10/16], Loss: 1.4374, Accuracy: 36.88%\n",
      "Epoch [532/1000], Batch [10/16], Loss: 1.4491, Accuracy: 38.12%\n",
      "Epoch [533/1000], Batch [10/16], Loss: 1.4357, Accuracy: 35.78%\n",
      "Epoch [534/1000], Batch [10/16], Loss: 1.4481, Accuracy: 36.72%\n",
      "Epoch [535/1000], Batch [10/16], Loss: 1.4262, Accuracy: 37.50%\n",
      "Epoch [536/1000], Batch [10/16], Loss: 1.4582, Accuracy: 34.69%\n",
      "Epoch [537/1000], Batch [10/16], Loss: 1.4314, Accuracy: 36.56%\n",
      "Epoch [538/1000], Batch [10/16], Loss: 1.4470, Accuracy: 34.69%\n",
      "Epoch [539/1000], Batch [10/16], Loss: 1.4426, Accuracy: 36.88%\n",
      "Epoch [540/1000], Batch [10/16], Loss: 1.4247, Accuracy: 37.66%\n",
      "Epoch [541/1000], Batch [10/16], Loss: 1.4313, Accuracy: 37.50%\n",
      "Epoch [542/1000], Batch [10/16], Loss: 1.4280, Accuracy: 37.50%\n",
      "Epoch [543/1000], Batch [10/16], Loss: 1.4225, Accuracy: 39.06%\n",
      "Epoch [544/1000], Batch [10/16], Loss: 1.4427, Accuracy: 36.25%\n",
      "Epoch [545/1000], Batch [10/16], Loss: 1.4196, Accuracy: 37.19%\n",
      "Epoch [546/1000], Batch [10/16], Loss: 1.4581, Accuracy: 35.62%\n",
      "Epoch [547/1000], Batch [10/16], Loss: 1.4399, Accuracy: 37.19%\n",
      "Epoch [548/1000], Batch [10/16], Loss: 1.4364, Accuracy: 36.72%\n",
      "Epoch [549/1000], Batch [10/16], Loss: 1.4676, Accuracy: 37.81%\n",
      "Epoch [550/1000], Batch [10/16], Loss: 1.4037, Accuracy: 39.22%\n",
      "Epoch [551/1000], Batch [10/16], Loss: 1.4290, Accuracy: 37.81%\n",
      "Epoch [552/1000], Batch [10/16], Loss: 1.4136, Accuracy: 38.91%\n",
      "Epoch [553/1000], Batch [10/16], Loss: 1.4516, Accuracy: 35.94%\n",
      "Epoch [554/1000], Batch [10/16], Loss: 1.4635, Accuracy: 34.69%\n",
      "Epoch [555/1000], Batch [10/16], Loss: 1.4360, Accuracy: 37.81%\n",
      "Epoch [556/1000], Batch [10/16], Loss: 1.4573, Accuracy: 34.06%\n",
      "Epoch [557/1000], Batch [10/16], Loss: 1.4178, Accuracy: 37.34%\n",
      "Epoch [558/1000], Batch [10/16], Loss: 1.4351, Accuracy: 35.62%\n",
      "Epoch [559/1000], Batch [10/16], Loss: 1.4205, Accuracy: 38.91%\n",
      "Epoch [560/1000], Batch [10/16], Loss: 1.4471, Accuracy: 35.78%\n",
      "Epoch [561/1000], Batch [10/16], Loss: 1.4501, Accuracy: 35.94%\n",
      "Epoch [562/1000], Batch [10/16], Loss: 1.4405, Accuracy: 37.66%\n",
      "Epoch [563/1000], Batch [10/16], Loss: 1.4374, Accuracy: 37.34%\n",
      "Epoch [564/1000], Batch [10/16], Loss: 1.4324, Accuracy: 37.19%\n",
      "Epoch [565/1000], Batch [10/16], Loss: 1.4390, Accuracy: 37.50%\n",
      "Epoch [566/1000], Batch [10/16], Loss: 1.4343, Accuracy: 35.94%\n",
      "Epoch [567/1000], Batch [10/16], Loss: 1.4226, Accuracy: 37.66%\n",
      "Epoch [568/1000], Batch [10/16], Loss: 1.4373, Accuracy: 36.88%\n",
      "Epoch [569/1000], Batch [10/16], Loss: 1.4327, Accuracy: 37.03%\n",
      "Epoch [570/1000], Batch [10/16], Loss: 1.4229, Accuracy: 36.72%\n",
      "Epoch [571/1000], Batch [10/16], Loss: 1.4354, Accuracy: 37.03%\n",
      "Epoch [572/1000], Batch [10/16], Loss: 1.4257, Accuracy: 38.12%\n",
      "Epoch [573/1000], Batch [10/16], Loss: 1.4365, Accuracy: 36.56%\n",
      "Epoch [574/1000], Batch [10/16], Loss: 1.4252, Accuracy: 38.91%\n",
      "Epoch [575/1000], Batch [10/16], Loss: 1.4402, Accuracy: 37.03%\n",
      "Epoch [576/1000], Batch [10/16], Loss: 1.4436, Accuracy: 38.59%\n",
      "Epoch [577/1000], Batch [10/16], Loss: 1.4258, Accuracy: 37.97%\n",
      "Epoch [578/1000], Batch [10/16], Loss: 1.4123, Accuracy: 38.91%\n",
      "Epoch [579/1000], Batch [10/16], Loss: 1.4128, Accuracy: 37.81%\n",
      "Epoch [580/1000], Batch [10/16], Loss: 1.4464, Accuracy: 35.78%\n",
      "Epoch [581/1000], Batch [10/16], Loss: 1.4493, Accuracy: 35.78%\n",
      "Epoch [582/1000], Batch [10/16], Loss: 1.4217, Accuracy: 38.75%\n",
      "Epoch [583/1000], Batch [10/16], Loss: 1.4309, Accuracy: 37.81%\n",
      "Epoch [584/1000], Batch [10/16], Loss: 1.4383, Accuracy: 35.62%\n",
      "Epoch [585/1000], Batch [10/16], Loss: 1.4427, Accuracy: 37.03%\n",
      "Epoch [586/1000], Batch [10/16], Loss: 1.4411, Accuracy: 37.03%\n",
      "Epoch [587/1000], Batch [10/16], Loss: 1.4500, Accuracy: 37.50%\n",
      "Epoch [588/1000], Batch [10/16], Loss: 1.4416, Accuracy: 35.16%\n",
      "Epoch [589/1000], Batch [10/16], Loss: 1.4282, Accuracy: 37.66%\n",
      "Epoch [590/1000], Batch [10/16], Loss: 1.4283, Accuracy: 37.34%\n",
      "Epoch [591/1000], Batch [10/16], Loss: 1.4405, Accuracy: 36.56%\n",
      "Epoch [592/1000], Batch [10/16], Loss: 1.4581, Accuracy: 35.16%\n",
      "Epoch [593/1000], Batch [10/16], Loss: 1.4429, Accuracy: 36.56%\n",
      "Epoch [594/1000], Batch [10/16], Loss: 1.4354, Accuracy: 37.50%\n",
      "Epoch [595/1000], Batch [10/16], Loss: 1.4410, Accuracy: 37.03%\n",
      "Epoch [596/1000], Batch [10/16], Loss: 1.4337, Accuracy: 36.72%\n",
      "Epoch [597/1000], Batch [10/16], Loss: 1.4468, Accuracy: 36.88%\n",
      "Epoch [598/1000], Batch [10/16], Loss: 1.4269, Accuracy: 38.28%\n",
      "Epoch [599/1000], Batch [10/16], Loss: 1.4411, Accuracy: 36.88%\n",
      "Epoch [600/1000], Batch [10/16], Loss: 1.4546, Accuracy: 36.25%\n",
      "Epoch [601/1000], Batch [10/16], Loss: 1.4489, Accuracy: 35.78%\n",
      "Epoch [602/1000], Batch [10/16], Loss: 1.4092, Accuracy: 37.97%\n",
      "Epoch [603/1000], Batch [10/16], Loss: 1.4489, Accuracy: 36.09%\n",
      "Epoch [604/1000], Batch [10/16], Loss: 1.4298, Accuracy: 37.97%\n",
      "Epoch [605/1000], Batch [10/16], Loss: 1.4303, Accuracy: 38.75%\n",
      "Epoch [606/1000], Batch [10/16], Loss: 1.4156, Accuracy: 38.44%\n",
      "Epoch [607/1000], Batch [10/16], Loss: 1.4070, Accuracy: 39.69%\n",
      "Epoch [608/1000], Batch [10/16], Loss: 1.4381, Accuracy: 36.56%\n",
      "Epoch [609/1000], Batch [10/16], Loss: 1.4327, Accuracy: 37.66%\n",
      "Epoch [610/1000], Batch [10/16], Loss: 1.4350, Accuracy: 36.41%\n",
      "Epoch [611/1000], Batch [10/16], Loss: 1.4047, Accuracy: 39.06%\n",
      "Epoch [612/1000], Batch [10/16], Loss: 1.4312, Accuracy: 38.12%\n",
      "Epoch [613/1000], Batch [10/16], Loss: 1.4404, Accuracy: 37.81%\n",
      "Epoch [614/1000], Batch [10/16], Loss: 1.4357, Accuracy: 38.28%\n",
      "Epoch [615/1000], Batch [10/16], Loss: 1.4380, Accuracy: 38.59%\n",
      "Epoch [616/1000], Batch [10/16], Loss: 1.4238, Accuracy: 38.28%\n",
      "Epoch [617/1000], Batch [10/16], Loss: 1.4496, Accuracy: 35.00%\n",
      "Epoch [618/1000], Batch [10/16], Loss: 1.4422, Accuracy: 37.19%\n",
      "Epoch [619/1000], Batch [10/16], Loss: 1.4426, Accuracy: 37.97%\n",
      "Epoch [620/1000], Batch [10/16], Loss: 1.4451, Accuracy: 35.00%\n",
      "Epoch [621/1000], Batch [10/16], Loss: 1.4334, Accuracy: 37.34%\n",
      "Epoch [622/1000], Batch [10/16], Loss: 1.4543, Accuracy: 35.78%\n",
      "Epoch [623/1000], Batch [10/16], Loss: 1.4306, Accuracy: 37.19%\n",
      "Epoch [624/1000], Batch [10/16], Loss: 1.4274, Accuracy: 39.38%\n",
      "Epoch [625/1000], Batch [10/16], Loss: 1.4150, Accuracy: 39.53%\n",
      "Epoch [626/1000], Batch [10/16], Loss: 1.4451, Accuracy: 35.78%\n",
      "Epoch [627/1000], Batch [10/16], Loss: 1.4325, Accuracy: 37.81%\n",
      "Epoch [628/1000], Batch [10/16], Loss: 1.4406, Accuracy: 37.97%\n",
      "Epoch [629/1000], Batch [10/16], Loss: 1.4448, Accuracy: 35.62%\n",
      "Epoch [630/1000], Batch [10/16], Loss: 1.4521, Accuracy: 36.09%\n",
      "Epoch [631/1000], Batch [10/16], Loss: 1.4379, Accuracy: 37.50%\n",
      "Epoch [632/1000], Batch [10/16], Loss: 1.4393, Accuracy: 38.59%\n",
      "Epoch [633/1000], Batch [10/16], Loss: 1.4172, Accuracy: 38.12%\n",
      "Epoch [634/1000], Batch [10/16], Loss: 1.4226, Accuracy: 38.28%\n",
      "Epoch [635/1000], Batch [10/16], Loss: 1.4097, Accuracy: 38.44%\n",
      "Epoch [636/1000], Batch [10/16], Loss: 1.4305, Accuracy: 36.41%\n",
      "Epoch [637/1000], Batch [10/16], Loss: 1.4279, Accuracy: 39.84%\n",
      "Epoch [638/1000], Batch [10/16], Loss: 1.4121, Accuracy: 40.16%\n",
      "Epoch [639/1000], Batch [10/16], Loss: 1.4115, Accuracy: 40.00%\n",
      "Epoch [640/1000], Batch [10/16], Loss: 1.4287, Accuracy: 37.34%\n",
      "Epoch [641/1000], Batch [10/16], Loss: 1.4215, Accuracy: 38.44%\n",
      "Epoch [642/1000], Batch [10/16], Loss: 1.4440, Accuracy: 37.34%\n",
      "Epoch [643/1000], Batch [10/16], Loss: 1.4184, Accuracy: 38.59%\n",
      "Epoch [644/1000], Batch [10/16], Loss: 1.4165, Accuracy: 39.38%\n",
      "Epoch [645/1000], Batch [10/16], Loss: 1.4128, Accuracy: 38.59%\n",
      "Epoch [646/1000], Batch [10/16], Loss: 1.4459, Accuracy: 37.81%\n",
      "Epoch [647/1000], Batch [10/16], Loss: 1.4454, Accuracy: 37.50%\n",
      "Epoch [648/1000], Batch [10/16], Loss: 1.4340, Accuracy: 37.66%\n",
      "Epoch [649/1000], Batch [10/16], Loss: 1.4438, Accuracy: 38.12%\n",
      "Epoch [650/1000], Batch [10/16], Loss: 1.4461, Accuracy: 36.56%\n",
      "Epoch [651/1000], Batch [10/16], Loss: 1.4381, Accuracy: 38.75%\n",
      "Epoch [652/1000], Batch [10/16], Loss: 1.4405, Accuracy: 36.25%\n",
      "Epoch [653/1000], Batch [10/16], Loss: 1.4364, Accuracy: 36.88%\n",
      "Epoch [654/1000], Batch [10/16], Loss: 1.4379, Accuracy: 36.88%\n",
      "Epoch [655/1000], Batch [10/16], Loss: 1.4290, Accuracy: 37.34%\n",
      "Epoch [656/1000], Batch [10/16], Loss: 1.4221, Accuracy: 39.22%\n",
      "Epoch [657/1000], Batch [10/16], Loss: 1.4382, Accuracy: 38.75%\n",
      "Epoch [658/1000], Batch [10/16], Loss: 1.4367, Accuracy: 37.50%\n",
      "Epoch [659/1000], Batch [10/16], Loss: 1.4396, Accuracy: 37.03%\n",
      "Epoch [660/1000], Batch [10/16], Loss: 1.4413, Accuracy: 36.72%\n",
      "Epoch [661/1000], Batch [10/16], Loss: 1.4444, Accuracy: 36.09%\n",
      "Epoch [662/1000], Batch [10/16], Loss: 1.4581, Accuracy: 36.09%\n",
      "Epoch [663/1000], Batch [10/16], Loss: 1.4283, Accuracy: 39.06%\n",
      "Epoch [664/1000], Batch [10/16], Loss: 1.4282, Accuracy: 37.03%\n",
      "Epoch [665/1000], Batch [10/16], Loss: 1.4358, Accuracy: 37.66%\n",
      "Epoch [666/1000], Batch [10/16], Loss: 1.4217, Accuracy: 38.12%\n",
      "Epoch [667/1000], Batch [10/16], Loss: 1.4359, Accuracy: 37.03%\n",
      "Epoch [668/1000], Batch [10/16], Loss: 1.4180, Accuracy: 38.75%\n",
      "Epoch [669/1000], Batch [10/16], Loss: 1.4300, Accuracy: 37.97%\n",
      "Epoch [670/1000], Batch [10/16], Loss: 1.4398, Accuracy: 37.34%\n",
      "Epoch [671/1000], Batch [10/16], Loss: 1.4146, Accuracy: 39.69%\n",
      "Epoch [672/1000], Batch [10/16], Loss: 1.4204, Accuracy: 38.44%\n",
      "Epoch [673/1000], Batch [10/16], Loss: 1.4334, Accuracy: 37.03%\n",
      "Epoch [674/1000], Batch [10/16], Loss: 1.4352, Accuracy: 38.28%\n",
      "Epoch [675/1000], Batch [10/16], Loss: 1.4446, Accuracy: 37.81%\n",
      "Epoch [676/1000], Batch [10/16], Loss: 1.4350, Accuracy: 39.53%\n",
      "Epoch [677/1000], Batch [10/16], Loss: 1.4372, Accuracy: 37.97%\n",
      "Epoch [678/1000], Batch [10/16], Loss: 1.4373, Accuracy: 37.34%\n",
      "Epoch [679/1000], Batch [10/16], Loss: 1.4318, Accuracy: 37.03%\n",
      "Epoch [680/1000], Batch [10/16], Loss: 1.4369, Accuracy: 37.81%\n",
      "Epoch [681/1000], Batch [10/16], Loss: 1.4448, Accuracy: 38.12%\n",
      "Epoch [682/1000], Batch [10/16], Loss: 1.4409, Accuracy: 37.34%\n",
      "Epoch [683/1000], Batch [10/16], Loss: 1.4117, Accuracy: 40.00%\n",
      "Epoch [684/1000], Batch [10/16], Loss: 1.4739, Accuracy: 35.78%\n",
      "Epoch [685/1000], Batch [10/16], Loss: 1.4123, Accuracy: 38.12%\n",
      "Epoch [686/1000], Batch [10/16], Loss: 1.4394, Accuracy: 37.97%\n",
      "Epoch [687/1000], Batch [10/16], Loss: 1.4176, Accuracy: 37.97%\n",
      "Epoch [688/1000], Batch [10/16], Loss: 1.4138, Accuracy: 38.59%\n",
      "Epoch [689/1000], Batch [10/16], Loss: 1.4314, Accuracy: 38.75%\n",
      "Epoch [690/1000], Batch [10/16], Loss: 1.4260, Accuracy: 38.44%\n",
      "Epoch [691/1000], Batch [10/16], Loss: 1.4250, Accuracy: 37.81%\n",
      "Epoch [692/1000], Batch [10/16], Loss: 1.4554, Accuracy: 36.09%\n",
      "Epoch [693/1000], Batch [10/16], Loss: 1.4211, Accuracy: 38.44%\n",
      "Epoch [694/1000], Batch [10/16], Loss: 1.4292, Accuracy: 38.91%\n",
      "Epoch [695/1000], Batch [10/16], Loss: 1.4423, Accuracy: 37.50%\n",
      "Epoch [696/1000], Batch [10/16], Loss: 1.4268, Accuracy: 37.19%\n",
      "Epoch [697/1000], Batch [10/16], Loss: 1.4101, Accuracy: 38.28%\n",
      "Epoch [698/1000], Batch [10/16], Loss: 1.4408, Accuracy: 37.34%\n",
      "Epoch [699/1000], Batch [10/16], Loss: 1.4464, Accuracy: 37.03%\n",
      "Epoch [700/1000], Batch [10/16], Loss: 1.4199, Accuracy: 39.38%\n",
      "Epoch [701/1000], Batch [10/16], Loss: 1.4150, Accuracy: 38.59%\n",
      "Epoch [702/1000], Batch [10/16], Loss: 1.4398, Accuracy: 37.19%\n",
      "Epoch [703/1000], Batch [10/16], Loss: 1.4292, Accuracy: 38.12%\n",
      "Epoch [704/1000], Batch [10/16], Loss: 1.4465, Accuracy: 36.56%\n",
      "Epoch [705/1000], Batch [10/16], Loss: 1.4405, Accuracy: 36.88%\n",
      "Epoch [706/1000], Batch [10/16], Loss: 1.3941, Accuracy: 40.16%\n",
      "Epoch [707/1000], Batch [10/16], Loss: 1.4149, Accuracy: 38.75%\n",
      "Epoch [708/1000], Batch [10/16], Loss: 1.4315, Accuracy: 38.59%\n",
      "Epoch [709/1000], Batch [10/16], Loss: 1.4459, Accuracy: 35.78%\n",
      "Epoch [710/1000], Batch [10/16], Loss: 1.4389, Accuracy: 37.66%\n",
      "Epoch [711/1000], Batch [10/16], Loss: 1.4390, Accuracy: 36.41%\n",
      "Epoch [712/1000], Batch [10/16], Loss: 1.4478, Accuracy: 35.00%\n",
      "Epoch [713/1000], Batch [10/16], Loss: 1.4370, Accuracy: 37.19%\n",
      "Epoch [714/1000], Batch [10/16], Loss: 1.4059, Accuracy: 39.22%\n",
      "Epoch [715/1000], Batch [10/16], Loss: 1.4197, Accuracy: 38.28%\n",
      "Epoch [716/1000], Batch [10/16], Loss: 1.4373, Accuracy: 35.94%\n",
      "Epoch [717/1000], Batch [10/16], Loss: 1.4266, Accuracy: 37.34%\n",
      "Epoch [718/1000], Batch [10/16], Loss: 1.4175, Accuracy: 37.34%\n",
      "Epoch [719/1000], Batch [10/16], Loss: 1.4153, Accuracy: 39.38%\n",
      "Epoch [720/1000], Batch [10/16], Loss: 1.4284, Accuracy: 38.12%\n",
      "Epoch [721/1000], Batch [10/16], Loss: 1.4153, Accuracy: 38.28%\n",
      "Epoch [722/1000], Batch [10/16], Loss: 1.4351, Accuracy: 37.03%\n",
      "Epoch [723/1000], Batch [10/16], Loss: 1.4383, Accuracy: 37.34%\n",
      "Epoch [724/1000], Batch [10/16], Loss: 1.4009, Accuracy: 39.22%\n",
      "Epoch [725/1000], Batch [10/16], Loss: 1.4179, Accuracy: 38.44%\n",
      "Epoch [726/1000], Batch [10/16], Loss: 1.4423, Accuracy: 37.81%\n",
      "Epoch [727/1000], Batch [10/16], Loss: 1.4187, Accuracy: 37.34%\n",
      "Epoch [728/1000], Batch [10/16], Loss: 1.4393, Accuracy: 35.94%\n",
      "Epoch [729/1000], Batch [10/16], Loss: 1.4273, Accuracy: 36.41%\n",
      "Epoch [730/1000], Batch [10/16], Loss: 1.4216, Accuracy: 38.91%\n",
      "Epoch [731/1000], Batch [10/16], Loss: 1.4175, Accuracy: 40.47%\n",
      "Epoch [732/1000], Batch [10/16], Loss: 1.4288, Accuracy: 37.50%\n",
      "Epoch [733/1000], Batch [10/16], Loss: 1.4045, Accuracy: 38.28%\n",
      "Epoch [734/1000], Batch [10/16], Loss: 1.4255, Accuracy: 38.12%\n",
      "Epoch [735/1000], Batch [10/16], Loss: 1.4543, Accuracy: 37.50%\n",
      "Epoch [736/1000], Batch [10/16], Loss: 1.4195, Accuracy: 39.06%\n",
      "Epoch [737/1000], Batch [10/16], Loss: 1.4213, Accuracy: 36.41%\n",
      "Epoch [738/1000], Batch [10/16], Loss: 1.4367, Accuracy: 35.62%\n",
      "Epoch [739/1000], Batch [10/16], Loss: 1.4146, Accuracy: 40.00%\n",
      "Epoch [740/1000], Batch [10/16], Loss: 1.4408, Accuracy: 36.72%\n",
      "Epoch [741/1000], Batch [10/16], Loss: 1.4429, Accuracy: 37.03%\n",
      "Epoch [742/1000], Batch [10/16], Loss: 1.4482, Accuracy: 36.56%\n",
      "Epoch [743/1000], Batch [10/16], Loss: 1.4362, Accuracy: 36.88%\n",
      "Epoch [744/1000], Batch [10/16], Loss: 1.4272, Accuracy: 38.75%\n",
      "Epoch [745/1000], Batch [10/16], Loss: 1.4398, Accuracy: 37.66%\n",
      "Epoch [746/1000], Batch [10/16], Loss: 1.4504, Accuracy: 35.31%\n",
      "Epoch [747/1000], Batch [10/16], Loss: 1.4425, Accuracy: 36.09%\n",
      "Epoch [748/1000], Batch [10/16], Loss: 1.4157, Accuracy: 38.91%\n",
      "Epoch [749/1000], Batch [10/16], Loss: 1.4305, Accuracy: 37.50%\n",
      "Epoch [750/1000], Batch [10/16], Loss: 1.4409, Accuracy: 35.94%\n",
      "Epoch [751/1000], Batch [10/16], Loss: 1.4391, Accuracy: 37.50%\n",
      "Epoch [752/1000], Batch [10/16], Loss: 1.4276, Accuracy: 36.09%\n",
      "Epoch [753/1000], Batch [10/16], Loss: 1.4143, Accuracy: 38.75%\n",
      "Epoch [754/1000], Batch [10/16], Loss: 1.4168, Accuracy: 38.44%\n",
      "Epoch [755/1000], Batch [10/16], Loss: 1.4113, Accuracy: 38.75%\n",
      "Epoch [756/1000], Batch [10/16], Loss: 1.4374, Accuracy: 38.12%\n",
      "Epoch [757/1000], Batch [10/16], Loss: 1.4059, Accuracy: 39.22%\n",
      "Epoch [758/1000], Batch [10/16], Loss: 1.4344, Accuracy: 37.34%\n",
      "Epoch [759/1000], Batch [10/16], Loss: 1.4480, Accuracy: 37.50%\n",
      "Epoch [760/1000], Batch [10/16], Loss: 1.4470, Accuracy: 37.50%\n",
      "Epoch [761/1000], Batch [10/16], Loss: 1.4109, Accuracy: 37.81%\n",
      "Epoch [762/1000], Batch [10/16], Loss: 1.4222, Accuracy: 38.75%\n",
      "Epoch [763/1000], Batch [10/16], Loss: 1.4295, Accuracy: 39.06%\n",
      "Epoch [764/1000], Batch [10/16], Loss: 1.4281, Accuracy: 38.44%\n",
      "Epoch [765/1000], Batch [10/16], Loss: 1.4423, Accuracy: 36.72%\n",
      "Epoch [766/1000], Batch [10/16], Loss: 1.4518, Accuracy: 36.56%\n",
      "Epoch [767/1000], Batch [10/16], Loss: 1.4202, Accuracy: 37.34%\n",
      "Epoch [768/1000], Batch [10/16], Loss: 1.4216, Accuracy: 38.75%\n",
      "Epoch [769/1000], Batch [10/16], Loss: 1.4431, Accuracy: 36.88%\n",
      "Epoch [770/1000], Batch [10/16], Loss: 1.4357, Accuracy: 37.03%\n",
      "Epoch [771/1000], Batch [10/16], Loss: 1.4511, Accuracy: 36.88%\n",
      "Epoch [772/1000], Batch [10/16], Loss: 1.4434, Accuracy: 35.47%\n",
      "Epoch [773/1000], Batch [10/16], Loss: 1.4340, Accuracy: 37.66%\n",
      "Epoch [774/1000], Batch [10/16], Loss: 1.4333, Accuracy: 39.38%\n",
      "Epoch [775/1000], Batch [10/16], Loss: 1.4279, Accuracy: 36.88%\n",
      "Epoch [776/1000], Batch [10/16], Loss: 1.4465, Accuracy: 37.50%\n",
      "Epoch [777/1000], Batch [10/16], Loss: 1.4054, Accuracy: 39.69%\n",
      "Epoch [778/1000], Batch [10/16], Loss: 1.4198, Accuracy: 38.91%\n",
      "Epoch [779/1000], Batch [10/16], Loss: 1.3994, Accuracy: 39.22%\n",
      "Epoch [780/1000], Batch [10/16], Loss: 1.4089, Accuracy: 38.28%\n",
      "Epoch [781/1000], Batch [10/16], Loss: 1.4255, Accuracy: 38.75%\n",
      "Epoch [782/1000], Batch [10/16], Loss: 1.4164, Accuracy: 38.59%\n",
      "Epoch [783/1000], Batch [10/16], Loss: 1.4298, Accuracy: 38.59%\n",
      "Epoch [784/1000], Batch [10/16], Loss: 1.4436, Accuracy: 37.03%\n",
      "Epoch [785/1000], Batch [10/16], Loss: 1.4246, Accuracy: 39.06%\n",
      "Epoch [786/1000], Batch [10/16], Loss: 1.4261, Accuracy: 38.28%\n",
      "Epoch [787/1000], Batch [10/16], Loss: 1.4114, Accuracy: 39.22%\n",
      "Epoch [788/1000], Batch [10/16], Loss: 1.4277, Accuracy: 38.59%\n",
      "Epoch [789/1000], Batch [10/16], Loss: 1.4305, Accuracy: 38.12%\n",
      "Epoch [790/1000], Batch [10/16], Loss: 1.4232, Accuracy: 39.06%\n",
      "Epoch [791/1000], Batch [10/16], Loss: 1.4353, Accuracy: 36.56%\n",
      "Epoch [792/1000], Batch [10/16], Loss: 1.4388, Accuracy: 37.03%\n",
      "Epoch [793/1000], Batch [10/16], Loss: 1.4256, Accuracy: 38.59%\n",
      "Epoch [794/1000], Batch [10/16], Loss: 1.4494, Accuracy: 36.41%\n",
      "Epoch [795/1000], Batch [10/16], Loss: 1.4258, Accuracy: 38.44%\n",
      "Epoch [796/1000], Batch [10/16], Loss: 1.3999, Accuracy: 39.38%\n",
      "Epoch [797/1000], Batch [10/16], Loss: 1.4184, Accuracy: 37.50%\n",
      "Epoch [798/1000], Batch [10/16], Loss: 1.4018, Accuracy: 39.06%\n",
      "Epoch [799/1000], Batch [10/16], Loss: 1.4214, Accuracy: 40.00%\n",
      "Epoch [800/1000], Batch [10/16], Loss: 1.4079, Accuracy: 40.16%\n",
      "Epoch [801/1000], Batch [10/16], Loss: 1.4326, Accuracy: 38.12%\n",
      "Epoch [802/1000], Batch [10/16], Loss: 1.4240, Accuracy: 38.75%\n",
      "Epoch [803/1000], Batch [10/16], Loss: 1.4231, Accuracy: 39.53%\n",
      "Epoch [804/1000], Batch [10/16], Loss: 1.4150, Accuracy: 38.12%\n",
      "Epoch [805/1000], Batch [10/16], Loss: 1.4112, Accuracy: 37.81%\n",
      "Epoch [806/1000], Batch [10/16], Loss: 1.4334, Accuracy: 37.97%\n",
      "Epoch [807/1000], Batch [10/16], Loss: 1.4282, Accuracy: 37.97%\n",
      "Epoch [808/1000], Batch [10/16], Loss: 1.4212, Accuracy: 38.12%\n",
      "Epoch [809/1000], Batch [10/16], Loss: 1.4281, Accuracy: 38.28%\n",
      "Epoch [810/1000], Batch [10/16], Loss: 1.4215, Accuracy: 37.34%\n",
      "Epoch [811/1000], Batch [10/16], Loss: 1.4280, Accuracy: 38.44%\n",
      "Epoch [812/1000], Batch [10/16], Loss: 1.4247, Accuracy: 38.59%\n",
      "Epoch [813/1000], Batch [10/16], Loss: 1.4128, Accuracy: 39.38%\n",
      "Epoch [814/1000], Batch [10/16], Loss: 1.4117, Accuracy: 37.97%\n",
      "Epoch [815/1000], Batch [10/16], Loss: 1.4212, Accuracy: 38.91%\n",
      "Epoch [816/1000], Batch [10/16], Loss: 1.4159, Accuracy: 38.59%\n",
      "Epoch [817/1000], Batch [10/16], Loss: 1.4563, Accuracy: 36.25%\n",
      "Epoch [818/1000], Batch [10/16], Loss: 1.4184, Accuracy: 39.69%\n",
      "Epoch [819/1000], Batch [10/16], Loss: 1.4312, Accuracy: 37.97%\n",
      "Epoch [820/1000], Batch [10/16], Loss: 1.4051, Accuracy: 41.25%\n",
      "Epoch [821/1000], Batch [10/16], Loss: 1.4089, Accuracy: 39.53%\n",
      "Epoch [822/1000], Batch [10/16], Loss: 1.4308, Accuracy: 37.19%\n",
      "Epoch [823/1000], Batch [10/16], Loss: 1.4300, Accuracy: 37.66%\n",
      "Epoch [824/1000], Batch [10/16], Loss: 1.4053, Accuracy: 39.69%\n",
      "Epoch [825/1000], Batch [10/16], Loss: 1.4231, Accuracy: 36.41%\n",
      "Epoch [826/1000], Batch [10/16], Loss: 1.4259, Accuracy: 37.34%\n",
      "Epoch [827/1000], Batch [10/16], Loss: 1.4056, Accuracy: 40.31%\n",
      "Epoch [828/1000], Batch [10/16], Loss: 1.4231, Accuracy: 39.06%\n",
      "Epoch [829/1000], Batch [10/16], Loss: 1.4125, Accuracy: 39.38%\n",
      "Epoch [830/1000], Batch [10/16], Loss: 1.4138, Accuracy: 39.38%\n",
      "Epoch [831/1000], Batch [10/16], Loss: 1.4241, Accuracy: 37.81%\n",
      "Epoch [832/1000], Batch [10/16], Loss: 1.4211, Accuracy: 38.59%\n",
      "Epoch [833/1000], Batch [10/16], Loss: 1.4187, Accuracy: 38.91%\n",
      "Epoch [834/1000], Batch [10/16], Loss: 1.4176, Accuracy: 38.28%\n",
      "Epoch [835/1000], Batch [10/16], Loss: 1.4107, Accuracy: 39.38%\n",
      "Epoch [836/1000], Batch [10/16], Loss: 1.4325, Accuracy: 37.03%\n",
      "Epoch [837/1000], Batch [10/16], Loss: 1.4149, Accuracy: 39.38%\n",
      "Epoch [838/1000], Batch [10/16], Loss: 1.4331, Accuracy: 37.97%\n",
      "Epoch [839/1000], Batch [10/16], Loss: 1.4133, Accuracy: 39.84%\n",
      "Epoch [840/1000], Batch [10/16], Loss: 1.4206, Accuracy: 38.44%\n",
      "Epoch [841/1000], Batch [10/16], Loss: 1.4495, Accuracy: 35.47%\n",
      "Epoch [842/1000], Batch [10/16], Loss: 1.4073, Accuracy: 39.06%\n",
      "Epoch [843/1000], Batch [10/16], Loss: 1.4209, Accuracy: 39.22%\n",
      "Epoch [844/1000], Batch [10/16], Loss: 1.4118, Accuracy: 40.00%\n",
      "Epoch [845/1000], Batch [10/16], Loss: 1.3970, Accuracy: 39.38%\n",
      "Epoch [846/1000], Batch [10/16], Loss: 1.4300, Accuracy: 38.91%\n",
      "Epoch [847/1000], Batch [10/16], Loss: 1.4326, Accuracy: 38.44%\n",
      "Epoch [848/1000], Batch [10/16], Loss: 1.4188, Accuracy: 39.53%\n",
      "Epoch [849/1000], Batch [10/16], Loss: 1.4345, Accuracy: 38.12%\n",
      "Epoch [850/1000], Batch [10/16], Loss: 1.4141, Accuracy: 37.97%\n",
      "Epoch [851/1000], Batch [10/16], Loss: 1.4174, Accuracy: 39.69%\n",
      "Epoch [852/1000], Batch [10/16], Loss: 1.4384, Accuracy: 37.81%\n",
      "Epoch [853/1000], Batch [10/16], Loss: 1.4462, Accuracy: 38.12%\n",
      "Epoch [854/1000], Batch [10/16], Loss: 1.4203, Accuracy: 37.81%\n",
      "Epoch [855/1000], Batch [10/16], Loss: 1.4470, Accuracy: 37.03%\n",
      "Epoch [856/1000], Batch [10/16], Loss: 1.4325, Accuracy: 36.56%\n",
      "Epoch [857/1000], Batch [10/16], Loss: 1.4268, Accuracy: 38.59%\n",
      "Epoch [858/1000], Batch [10/16], Loss: 1.4273, Accuracy: 38.44%\n",
      "Epoch [859/1000], Batch [10/16], Loss: 1.4198, Accuracy: 38.59%\n",
      "Epoch [860/1000], Batch [10/16], Loss: 1.4307, Accuracy: 37.19%\n",
      "Epoch [861/1000], Batch [10/16], Loss: 1.4100, Accuracy: 38.44%\n",
      "Epoch [862/1000], Batch [10/16], Loss: 1.4361, Accuracy: 37.50%\n",
      "Epoch [863/1000], Batch [10/16], Loss: 1.4313, Accuracy: 39.06%\n",
      "Epoch [864/1000], Batch [10/16], Loss: 1.4345, Accuracy: 38.28%\n",
      "Epoch [865/1000], Batch [10/16], Loss: 1.4356, Accuracy: 36.88%\n",
      "Epoch [866/1000], Batch [10/16], Loss: 1.4183, Accuracy: 40.47%\n",
      "Epoch [867/1000], Batch [10/16], Loss: 1.4002, Accuracy: 38.75%\n",
      "Epoch [868/1000], Batch [10/16], Loss: 1.4472, Accuracy: 36.72%\n",
      "Epoch [869/1000], Batch [10/16], Loss: 1.4127, Accuracy: 40.16%\n",
      "Epoch [870/1000], Batch [10/16], Loss: 1.4145, Accuracy: 40.16%\n",
      "Epoch [871/1000], Batch [10/16], Loss: 1.4400, Accuracy: 37.19%\n",
      "Epoch [872/1000], Batch [10/16], Loss: 1.4158, Accuracy: 40.00%\n",
      "Epoch [873/1000], Batch [10/16], Loss: 1.4315, Accuracy: 38.59%\n",
      "Epoch [874/1000], Batch [10/16], Loss: 1.4056, Accuracy: 39.69%\n",
      "Epoch [875/1000], Batch [10/16], Loss: 1.4004, Accuracy: 40.31%\n",
      "Epoch [876/1000], Batch [10/16], Loss: 1.4120, Accuracy: 39.22%\n",
      "Epoch [877/1000], Batch [10/16], Loss: 1.4422, Accuracy: 36.72%\n",
      "Epoch [878/1000], Batch [10/16], Loss: 1.4075, Accuracy: 39.06%\n",
      "Epoch [879/1000], Batch [10/16], Loss: 1.4378, Accuracy: 37.34%\n",
      "Epoch [880/1000], Batch [10/16], Loss: 1.3853, Accuracy: 41.41%\n",
      "Epoch [881/1000], Batch [10/16], Loss: 1.4247, Accuracy: 39.38%\n",
      "Epoch [882/1000], Batch [10/16], Loss: 1.4304, Accuracy: 38.75%\n",
      "Epoch [883/1000], Batch [10/16], Loss: 1.4272, Accuracy: 37.03%\n",
      "Epoch [884/1000], Batch [10/16], Loss: 1.4330, Accuracy: 39.84%\n",
      "Epoch [885/1000], Batch [10/16], Loss: 1.4210, Accuracy: 39.53%\n",
      "Epoch [886/1000], Batch [10/16], Loss: 1.4274, Accuracy: 37.81%\n",
      "Epoch [887/1000], Batch [10/16], Loss: 1.4102, Accuracy: 40.62%\n",
      "Epoch [888/1000], Batch [10/16], Loss: 1.3936, Accuracy: 40.00%\n",
      "Epoch [889/1000], Batch [10/16], Loss: 1.4230, Accuracy: 38.44%\n",
      "Epoch [890/1000], Batch [10/16], Loss: 1.4126, Accuracy: 39.06%\n",
      "Epoch [891/1000], Batch [10/16], Loss: 1.4042, Accuracy: 41.72%\n",
      "Epoch [892/1000], Batch [10/16], Loss: 1.3959, Accuracy: 39.69%\n",
      "Epoch [893/1000], Batch [10/16], Loss: 1.4136, Accuracy: 39.06%\n",
      "Epoch [894/1000], Batch [10/16], Loss: 1.4050, Accuracy: 39.69%\n",
      "Epoch [895/1000], Batch [10/16], Loss: 1.3990, Accuracy: 39.22%\n",
      "Epoch [896/1000], Batch [10/16], Loss: 1.4285, Accuracy: 38.91%\n",
      "Epoch [897/1000], Batch [10/16], Loss: 1.4181, Accuracy: 38.28%\n",
      "Epoch [898/1000], Batch [10/16], Loss: 1.4153, Accuracy: 38.75%\n",
      "Epoch [899/1000], Batch [10/16], Loss: 1.4211, Accuracy: 39.38%\n",
      "Epoch [900/1000], Batch [10/16], Loss: 1.4131, Accuracy: 39.69%\n",
      "Epoch [901/1000], Batch [10/16], Loss: 1.4214, Accuracy: 37.66%\n",
      "Epoch [902/1000], Batch [10/16], Loss: 1.4152, Accuracy: 38.44%\n",
      "Epoch [903/1000], Batch [10/16], Loss: 1.4080, Accuracy: 39.53%\n",
      "Epoch [904/1000], Batch [10/16], Loss: 1.4266, Accuracy: 39.69%\n",
      "Epoch [905/1000], Batch [10/16], Loss: 1.4257, Accuracy: 36.56%\n",
      "Epoch [906/1000], Batch [10/16], Loss: 1.3765, Accuracy: 40.16%\n",
      "Epoch [907/1000], Batch [10/16], Loss: 1.4171, Accuracy: 40.16%\n",
      "Epoch [908/1000], Batch [10/16], Loss: 1.4180, Accuracy: 38.44%\n",
      "Epoch [909/1000], Batch [10/16], Loss: 1.4234, Accuracy: 38.28%\n",
      "Epoch [910/1000], Batch [10/16], Loss: 1.4428, Accuracy: 37.19%\n",
      "Epoch [911/1000], Batch [10/16], Loss: 1.4054, Accuracy: 40.31%\n",
      "Epoch [912/1000], Batch [10/16], Loss: 1.4036, Accuracy: 39.84%\n",
      "Epoch [913/1000], Batch [10/16], Loss: 1.4172, Accuracy: 38.44%\n",
      "Epoch [914/1000], Batch [10/16], Loss: 1.3844, Accuracy: 39.84%\n",
      "Epoch [915/1000], Batch [10/16], Loss: 1.4377, Accuracy: 37.81%\n",
      "Epoch [916/1000], Batch [10/16], Loss: 1.4431, Accuracy: 36.41%\n",
      "Epoch [917/1000], Batch [10/16], Loss: 1.4498, Accuracy: 35.62%\n",
      "Epoch [918/1000], Batch [10/16], Loss: 1.4234, Accuracy: 39.38%\n",
      "Epoch [919/1000], Batch [10/16], Loss: 1.4242, Accuracy: 38.75%\n",
      "Epoch [920/1000], Batch [10/16], Loss: 1.4202, Accuracy: 39.06%\n",
      "Epoch [921/1000], Batch [10/16], Loss: 1.4341, Accuracy: 37.66%\n",
      "Epoch [922/1000], Batch [10/16], Loss: 1.4218, Accuracy: 37.97%\n",
      "Epoch [923/1000], Batch [10/16], Loss: 1.4192, Accuracy: 38.91%\n",
      "Epoch [924/1000], Batch [10/16], Loss: 1.4055, Accuracy: 41.41%\n",
      "Epoch [925/1000], Batch [10/16], Loss: 1.4243, Accuracy: 40.00%\n",
      "Epoch [926/1000], Batch [10/16], Loss: 1.4002, Accuracy: 40.31%\n",
      "Epoch [927/1000], Batch [10/16], Loss: 1.4159, Accuracy: 39.69%\n",
      "Epoch [928/1000], Batch [10/16], Loss: 1.4166, Accuracy: 39.38%\n",
      "Epoch [929/1000], Batch [10/16], Loss: 1.4188, Accuracy: 37.34%\n",
      "Epoch [930/1000], Batch [10/16], Loss: 1.4498, Accuracy: 35.16%\n",
      "Epoch [931/1000], Batch [10/16], Loss: 1.4069, Accuracy: 39.22%\n",
      "Epoch [932/1000], Batch [10/16], Loss: 1.4028, Accuracy: 38.91%\n",
      "Epoch [933/1000], Batch [10/16], Loss: 1.4262, Accuracy: 38.44%\n",
      "Epoch [934/1000], Batch [10/16], Loss: 1.4194, Accuracy: 38.59%\n",
      "Epoch [935/1000], Batch [10/16], Loss: 1.4177, Accuracy: 39.38%\n",
      "Epoch [936/1000], Batch [10/16], Loss: 1.4270, Accuracy: 37.34%\n",
      "Epoch [937/1000], Batch [10/16], Loss: 1.4096, Accuracy: 40.00%\n",
      "Epoch [938/1000], Batch [10/16], Loss: 1.4188, Accuracy: 39.53%\n",
      "Epoch [939/1000], Batch [10/16], Loss: 1.4502, Accuracy: 36.09%\n",
      "Epoch [940/1000], Batch [10/16], Loss: 1.4150, Accuracy: 37.66%\n",
      "Epoch [941/1000], Batch [10/16], Loss: 1.4164, Accuracy: 39.38%\n",
      "Epoch [942/1000], Batch [10/16], Loss: 1.4145, Accuracy: 39.53%\n",
      "Epoch [943/1000], Batch [10/16], Loss: 1.4323, Accuracy: 37.34%\n",
      "Epoch [944/1000], Batch [10/16], Loss: 1.4163, Accuracy: 40.16%\n",
      "Epoch [945/1000], Batch [10/16], Loss: 1.3903, Accuracy: 40.94%\n",
      "Epoch [946/1000], Batch [10/16], Loss: 1.4271, Accuracy: 37.97%\n",
      "Epoch [947/1000], Batch [10/16], Loss: 1.4176, Accuracy: 39.22%\n",
      "Epoch [948/1000], Batch [10/16], Loss: 1.3934, Accuracy: 40.94%\n",
      "Epoch [949/1000], Batch [10/16], Loss: 1.4012, Accuracy: 40.94%\n",
      "Epoch [950/1000], Batch [10/16], Loss: 1.4119, Accuracy: 40.62%\n",
      "Epoch [951/1000], Batch [10/16], Loss: 1.4388, Accuracy: 37.97%\n",
      "Epoch [952/1000], Batch [10/16], Loss: 1.4082, Accuracy: 40.78%\n",
      "Epoch [953/1000], Batch [10/16], Loss: 1.4053, Accuracy: 39.22%\n",
      "Epoch [954/1000], Batch [10/16], Loss: 1.4334, Accuracy: 37.19%\n",
      "Epoch [955/1000], Batch [10/16], Loss: 1.4072, Accuracy: 40.16%\n",
      "Epoch [956/1000], Batch [10/16], Loss: 1.4321, Accuracy: 37.19%\n",
      "Epoch [957/1000], Batch [10/16], Loss: 1.4077, Accuracy: 40.78%\n",
      "Epoch [958/1000], Batch [10/16], Loss: 1.4031, Accuracy: 39.84%\n",
      "Epoch [959/1000], Batch [10/16], Loss: 1.4206, Accuracy: 38.59%\n",
      "Epoch [960/1000], Batch [10/16], Loss: 1.3930, Accuracy: 40.78%\n",
      "Epoch [961/1000], Batch [10/16], Loss: 1.4069, Accuracy: 40.47%\n",
      "Epoch [962/1000], Batch [10/16], Loss: 1.4268, Accuracy: 39.53%\n",
      "Epoch [963/1000], Batch [10/16], Loss: 1.3807, Accuracy: 41.25%\n",
      "Epoch [964/1000], Batch [10/16], Loss: 1.4131, Accuracy: 38.91%\n",
      "Epoch [965/1000], Batch [10/16], Loss: 1.4267, Accuracy: 39.06%\n",
      "Epoch [966/1000], Batch [10/16], Loss: 1.4214, Accuracy: 40.00%\n",
      "Epoch [967/1000], Batch [10/16], Loss: 1.4108, Accuracy: 39.06%\n",
      "Epoch [968/1000], Batch [10/16], Loss: 1.4467, Accuracy: 35.78%\n",
      "Epoch [969/1000], Batch [10/16], Loss: 1.4101, Accuracy: 40.00%\n",
      "Epoch [970/1000], Batch [10/16], Loss: 1.4217, Accuracy: 38.91%\n",
      "Epoch [971/1000], Batch [10/16], Loss: 1.3913, Accuracy: 41.25%\n",
      "Epoch [972/1000], Batch [10/16], Loss: 1.4213, Accuracy: 39.22%\n",
      "Epoch [973/1000], Batch [10/16], Loss: 1.4236, Accuracy: 38.12%\n",
      "Epoch [974/1000], Batch [10/16], Loss: 1.4039, Accuracy: 39.69%\n",
      "Epoch [975/1000], Batch [10/16], Loss: 1.4109, Accuracy: 38.91%\n",
      "Epoch [976/1000], Batch [10/16], Loss: 1.4214, Accuracy: 38.75%\n",
      "Epoch [977/1000], Batch [10/16], Loss: 1.4023, Accuracy: 39.84%\n",
      "Epoch [978/1000], Batch [10/16], Loss: 1.4178, Accuracy: 40.78%\n",
      "Epoch [979/1000], Batch [10/16], Loss: 1.3970, Accuracy: 40.62%\n",
      "Epoch [980/1000], Batch [10/16], Loss: 1.4283, Accuracy: 39.38%\n",
      "Epoch [981/1000], Batch [10/16], Loss: 1.4366, Accuracy: 38.59%\n",
      "Epoch [982/1000], Batch [10/16], Loss: 1.4212, Accuracy: 37.97%\n",
      "Epoch [983/1000], Batch [10/16], Loss: 1.3867, Accuracy: 40.78%\n",
      "Epoch [984/1000], Batch [10/16], Loss: 1.4057, Accuracy: 40.78%\n",
      "Epoch [985/1000], Batch [10/16], Loss: 1.4173, Accuracy: 39.84%\n",
      "Epoch [986/1000], Batch [10/16], Loss: 1.4073, Accuracy: 39.38%\n",
      "Epoch [987/1000], Batch [10/16], Loss: 1.4036, Accuracy: 40.16%\n",
      "Epoch [988/1000], Batch [10/16], Loss: 1.4068, Accuracy: 38.91%\n",
      "Epoch [989/1000], Batch [10/16], Loss: 1.3973, Accuracy: 40.16%\n",
      "Epoch [990/1000], Batch [10/16], Loss: 1.4223, Accuracy: 38.12%\n",
      "Epoch [991/1000], Batch [10/16], Loss: 1.4258, Accuracy: 38.59%\n",
      "Epoch [992/1000], Batch [10/16], Loss: 1.4324, Accuracy: 38.12%\n",
      "Epoch [993/1000], Batch [10/16], Loss: 1.3956, Accuracy: 41.72%\n",
      "Epoch [994/1000], Batch [10/16], Loss: 1.4106, Accuracy: 39.22%\n",
      "Epoch [995/1000], Batch [10/16], Loss: 1.4161, Accuracy: 38.75%\n",
      "Epoch [996/1000], Batch [10/16], Loss: 1.4113, Accuracy: 39.69%\n",
      "Epoch [997/1000], Batch [10/16], Loss: 1.4310, Accuracy: 38.44%\n",
      "Epoch [998/1000], Batch [10/16], Loss: 1.4035, Accuracy: 39.84%\n",
      "Epoch [999/1000], Batch [10/16], Loss: 1.4116, Accuracy: 39.06%\n",
      "Epoch [1000/1000], Batch [10/16], Loss: 1.4151, Accuracy: 38.75%\n",
      "Test Accuracy: 18.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the AttentionRNNCell (from your implementation)\n",
    "class AttentionRNNCell(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionRNNCell, self).__init__()\n",
    "        self.d_model = d_model  # Dimensionality of keys/queries/values\n",
    "\n",
    "    def forward_iterative(self, q, k, v, prev_a=None, prev_c=None, prev_m=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # Initialize previous states if not provided\n",
    "        if prev_a is None:\n",
    "            prev_a = torch.zeros(batch_size, self.d_model).to(k.device)\n",
    "        if prev_c is None:\n",
    "            prev_c = torch.zeros(batch_size, 1).to(k.device)\n",
    "        if prev_m is None:\n",
    "            prev_m = torch.full((batch_size, 1), 0).to(k.device)\n",
    "\n",
    "        # Compute scores s_k = q . k^T\n",
    "        s_k = torch.sum(q * k, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update m_k (max cumulative score)\n",
    "        m_k = torch.max(s_k, prev_m)\n",
    "\n",
    "        # Compute exp terms for stability\n",
    "        exp_term1 = torch.exp(prev_m - m_k)  # Shape: (batch_size, 1)\n",
    "        exp_term2 = torch.exp(s_k - m_k)    # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update a_k and c_k\n",
    "        a_k = prev_a * exp_term1 + v * exp_term2  # Shape: (batch_size, d_model)\n",
    "        c_k = prev_c * exp_term1 + exp_term2      # Shape: (batch_size, 1)\n",
    "\n",
    "        return a_k, c_k, m_k\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Define the full model\n",
    "class AttentionRNNClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(AttentionRNNClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Learnable query\n",
    "        self.query = nn.Parameter(torch.randn(1, d_model))\n",
    "\n",
    "        # Attention RNN Cell\n",
    "        self.attention_cell = AttentionRNNCell(d_model)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Expand query to match batch size\n",
    "        query = self.query.expand(batch_size, -1)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        a_k, c_k, m_k = None, None, None\n",
    "\n",
    "        # Process the sequence iteratively\n",
    "        for i in range(seq_len):\n",
    "            frame = x[:, i, :]  # Get the i-th frame in the sequence\n",
    "            a_k, c_k, m_k = self.attention_cell.forward_iterative(query, frame, frame, a_k, c_k, m_k)\n",
    "\n",
    "        # Final attention output\n",
    "        attention_output = a_k / c_k  # Shape: (batch_size, d_model)\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.fc(attention_output)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Simulate a more structured dataset\n",
    "class StructuredDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, d_model, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Simulate structured data (e.g., MFCCs)\n",
    "        self.data = torch.randn(num_samples, seq_len, d_model)  # Random audio frames\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))  # Random labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Configuration\n",
    "batch_size = 64\n",
    "seq_len = 20  # Number of audio frames per sequence\n",
    "d_model = 40  # Dimensionality of each audio frame (e.g., MFCCs)\n",
    "num_classes = 5  # Number of classes for classification\n",
    "num_samples = 1000  # Number of samples in the dataset\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = StructuredDataset(num_samples, seq_len, d_model, num_classes)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = AttentionRNNClassifier(d_model, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], \"\n",
    "                  f\"Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_dataset = StructuredDataset(num_samples=200, seq_len=seq_len, d_model=d_model, num_classes=num_classes)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        logits = model(data)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Batch [10/16], Loss: 1.4497, Accuracy: 38.44%\n",
      "Epoch [2/1000], Batch [10/16], Loss: 1.4349, Accuracy: 38.44%\n",
      "Epoch [3/1000], Batch [10/16], Loss: 1.4433, Accuracy: 35.62%\n",
      "Epoch [4/1000], Batch [10/16], Loss: 1.4231, Accuracy: 39.53%\n",
      "Epoch [5/1000], Batch [10/16], Loss: 1.4391, Accuracy: 36.25%\n",
      "Epoch [6/1000], Batch [10/16], Loss: 1.4261, Accuracy: 38.91%\n",
      "Epoch [7/1000], Batch [10/16], Loss: 1.4298, Accuracy: 37.97%\n",
      "Epoch [8/1000], Batch [10/16], Loss: 1.4267, Accuracy: 39.69%\n",
      "Epoch [9/1000], Batch [10/16], Loss: 1.4328, Accuracy: 37.66%\n",
      "Epoch [10/1000], Batch [10/16], Loss: 1.4390, Accuracy: 38.12%\n",
      "Epoch [11/1000], Batch [10/16], Loss: 1.4371, Accuracy: 38.12%\n",
      "Epoch [12/1000], Batch [10/16], Loss: 1.4130, Accuracy: 41.72%\n",
      "Epoch [13/1000], Batch [10/16], Loss: 1.4136, Accuracy: 39.84%\n",
      "Epoch [14/1000], Batch [10/16], Loss: 1.4257, Accuracy: 39.69%\n",
      "Epoch [15/1000], Batch [10/16], Loss: 1.4355, Accuracy: 38.44%\n",
      "Epoch [16/1000], Batch [10/16], Loss: 1.4409, Accuracy: 38.12%\n",
      "Epoch [17/1000], Batch [10/16], Loss: 1.4187, Accuracy: 40.16%\n",
      "Epoch [18/1000], Batch [10/16], Loss: 1.4323, Accuracy: 39.22%\n",
      "Epoch [19/1000], Batch [10/16], Loss: 1.4417, Accuracy: 38.59%\n",
      "Epoch [20/1000], Batch [10/16], Loss: 1.4157, Accuracy: 41.72%\n",
      "Epoch [21/1000], Batch [10/16], Loss: 1.4231, Accuracy: 40.47%\n",
      "Epoch [22/1000], Batch [10/16], Loss: 1.4371, Accuracy: 39.22%\n",
      "Epoch [23/1000], Batch [10/16], Loss: 1.4391, Accuracy: 38.28%\n",
      "Epoch [24/1000], Batch [10/16], Loss: 1.4211, Accuracy: 40.47%\n",
      "Epoch [25/1000], Batch [10/16], Loss: 1.4182, Accuracy: 39.06%\n",
      "Epoch [26/1000], Batch [10/16], Loss: 1.3993, Accuracy: 40.31%\n",
      "Epoch [27/1000], Batch [10/16], Loss: 1.4266, Accuracy: 38.12%\n",
      "Epoch [28/1000], Batch [10/16], Loss: 1.4406, Accuracy: 37.66%\n",
      "Epoch [29/1000], Batch [10/16], Loss: 1.4152, Accuracy: 40.31%\n",
      "Epoch [30/1000], Batch [10/16], Loss: 1.4380, Accuracy: 39.06%\n",
      "Epoch [31/1000], Batch [10/16], Loss: 1.4371, Accuracy: 40.16%\n",
      "Epoch [32/1000], Batch [10/16], Loss: 1.4249, Accuracy: 39.22%\n",
      "Epoch [33/1000], Batch [10/16], Loss: 1.4008, Accuracy: 41.72%\n",
      "Epoch [34/1000], Batch [10/16], Loss: 1.4315, Accuracy: 38.44%\n",
      "Epoch [35/1000], Batch [10/16], Loss: 1.4591, Accuracy: 37.34%\n",
      "Epoch [36/1000], Batch [10/16], Loss: 1.4436, Accuracy: 38.44%\n",
      "Epoch [37/1000], Batch [10/16], Loss: 1.3997, Accuracy: 41.09%\n",
      "Epoch [38/1000], Batch [10/16], Loss: 1.4162, Accuracy: 38.28%\n",
      "Epoch [39/1000], Batch [10/16], Loss: 1.4403, Accuracy: 38.75%\n",
      "Epoch [40/1000], Batch [10/16], Loss: 1.4335, Accuracy: 38.59%\n",
      "Epoch [41/1000], Batch [10/16], Loss: 1.4083, Accuracy: 39.22%\n",
      "Epoch [42/1000], Batch [10/16], Loss: 1.4483, Accuracy: 38.28%\n",
      "Epoch [43/1000], Batch [10/16], Loss: 1.4176, Accuracy: 40.16%\n",
      "Epoch [44/1000], Batch [10/16], Loss: 1.4240, Accuracy: 38.91%\n",
      "Epoch [45/1000], Batch [10/16], Loss: 1.4251, Accuracy: 39.22%\n",
      "Epoch [46/1000], Batch [10/16], Loss: 1.4377, Accuracy: 38.59%\n",
      "Epoch [47/1000], Batch [10/16], Loss: 1.4351, Accuracy: 38.12%\n",
      "Epoch [48/1000], Batch [10/16], Loss: 1.4148, Accuracy: 41.09%\n",
      "Epoch [49/1000], Batch [10/16], Loss: 1.4380, Accuracy: 38.91%\n",
      "Epoch [50/1000], Batch [10/16], Loss: 1.4243, Accuracy: 39.84%\n",
      "Test Accuracy: 16.00%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=10e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], \"\n",
    "                  f\"Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_dataset = StructuredDataset(num_samples=200, seq_len=seq_len, d_model=d_model, num_classes=num_classes)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        logits = model(data)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [10/16], Loss: 1.7429, Accuracy: 21.56%\n",
      "Epoch [2/50], Batch [10/16], Loss: 1.6825, Accuracy: 22.19%\n",
      "Epoch [3/50], Batch [10/16], Loss: 1.6673, Accuracy: 22.97%\n",
      "Epoch [4/50], Batch [10/16], Loss: 1.6670, Accuracy: 20.31%\n",
      "Epoch [5/50], Batch [10/16], Loss: 1.6576, Accuracy: 22.97%\n",
      "Epoch [6/50], Batch [10/16], Loss: 1.6322, Accuracy: 23.12%\n",
      "Epoch [7/50], Batch [10/16], Loss: 1.6390, Accuracy: 24.22%\n",
      "Epoch [8/50], Batch [10/16], Loss: 1.6380, Accuracy: 24.69%\n",
      "Epoch [9/50], Batch [10/16], Loss: 1.6154, Accuracy: 26.56%\n",
      "Epoch [10/50], Batch [10/16], Loss: 1.5985, Accuracy: 28.28%\n",
      "Epoch [11/50], Batch [10/16], Loss: 1.6093, Accuracy: 26.72%\n",
      "Epoch [12/50], Batch [10/16], Loss: 1.5923, Accuracy: 27.81%\n",
      "Epoch [13/50], Batch [10/16], Loss: 1.5914, Accuracy: 27.81%\n",
      "Epoch [14/50], Batch [10/16], Loss: 1.5752, Accuracy: 27.97%\n",
      "Epoch [15/50], Batch [10/16], Loss: 1.5776, Accuracy: 26.72%\n",
      "Epoch [16/50], Batch [10/16], Loss: 1.5691, Accuracy: 28.91%\n",
      "Epoch [17/50], Batch [10/16], Loss: 1.5654, Accuracy: 30.47%\n",
      "Epoch [18/50], Batch [10/16], Loss: 1.5441, Accuracy: 31.88%\n",
      "Epoch [19/50], Batch [10/16], Loss: 1.5679, Accuracy: 30.16%\n",
      "Epoch [20/50], Batch [10/16], Loss: 1.5521, Accuracy: 32.34%\n",
      "Epoch [21/50], Batch [10/16], Loss: 1.5533, Accuracy: 29.38%\n",
      "Epoch [22/50], Batch [10/16], Loss: 1.5672, Accuracy: 29.22%\n",
      "Epoch [23/50], Batch [10/16], Loss: 1.5525, Accuracy: 30.47%\n",
      "Epoch [24/50], Batch [10/16], Loss: 1.5476, Accuracy: 32.66%\n",
      "Epoch [25/50], Batch [10/16], Loss: 1.5441, Accuracy: 32.03%\n",
      "Epoch [26/50], Batch [10/16], Loss: 1.5444, Accuracy: 31.56%\n",
      "Epoch [27/50], Batch [10/16], Loss: 1.5370, Accuracy: 32.34%\n",
      "Epoch [28/50], Batch [10/16], Loss: 1.5419, Accuracy: 30.00%\n",
      "Epoch [29/50], Batch [10/16], Loss: 1.5354, Accuracy: 31.72%\n",
      "Epoch [30/50], Batch [10/16], Loss: 1.5253, Accuracy: 32.34%\n",
      "Epoch [31/50], Batch [10/16], Loss: 1.5402, Accuracy: 30.94%\n",
      "Epoch [32/50], Batch [10/16], Loss: 1.5312, Accuracy: 31.88%\n",
      "Epoch [33/50], Batch [10/16], Loss: 1.5345, Accuracy: 32.81%\n",
      "Epoch [34/50], Batch [10/16], Loss: 1.5282, Accuracy: 33.75%\n",
      "Epoch [35/50], Batch [10/16], Loss: 1.5283, Accuracy: 31.56%\n",
      "Epoch [36/50], Batch [10/16], Loss: 1.5345, Accuracy: 32.19%\n",
      "Epoch [37/50], Batch [10/16], Loss: 1.5376, Accuracy: 31.25%\n",
      "Epoch [38/50], Batch [10/16], Loss: 1.5255, Accuracy: 32.03%\n",
      "Epoch [39/50], Batch [10/16], Loss: 1.5302, Accuracy: 33.12%\n",
      "Epoch [40/50], Batch [10/16], Loss: 1.5306, Accuracy: 32.97%\n",
      "Epoch [41/50], Batch [10/16], Loss: 1.5153, Accuracy: 34.53%\n",
      "Epoch [42/50], Batch [10/16], Loss: 1.5353, Accuracy: 31.56%\n",
      "Epoch [43/50], Batch [10/16], Loss: 1.5143, Accuracy: 33.28%\n",
      "Epoch [44/50], Batch [10/16], Loss: 1.5270, Accuracy: 32.50%\n",
      "Epoch [45/50], Batch [10/16], Loss: 1.5126, Accuracy: 33.59%\n",
      "Epoch [46/50], Batch [10/16], Loss: 1.5178, Accuracy: 32.66%\n",
      "Epoch [47/50], Batch [10/16], Loss: 1.5293, Accuracy: 30.16%\n",
      "Epoch [48/50], Batch [10/16], Loss: 1.5108, Accuracy: 34.53%\n",
      "Epoch [49/50], Batch [10/16], Loss: 1.5155, Accuracy: 33.28%\n",
      "Epoch [50/50], Batch [10/16], Loss: 1.5054, Accuracy: 33.44%\n",
      "Test Accuracy: 16.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the ParallelAttentionScan module\n",
    "class ParallelAttentionScan(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParallelAttentionScan, self).__init__()\n",
    "\n",
    "    def combine(self, mA, uA, wA, mB, uB, wB):\n",
    "        mAB = torch.max(mA, mB)\n",
    "        expA = torch.exp(mA - mAB)\n",
    "        expB = torch.exp(mB - mAB)\n",
    "        uAB = uA * expA + uB * expB\n",
    "        wAB = wA * expA + wB * expB\n",
    "        return mAB, uAB, wAB\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        input_dim, batch_size, d_model = k.size()\n",
    "\n",
    "        # Initialize with the first token\n",
    "        s_initial = torch.sum(q * k[0], dim=-1, keepdim=True)\n",
    "        m = s_initial\n",
    "        u = torch.ones(batch_size, 1).to(k.device)\n",
    "        w = v[0]\n",
    "\n",
    "        # Iteratively combine the remaining tokens\n",
    "        for i in range(1, input_dim):\n",
    "            m, u, w = self.combine(m, u, w, k[i], torch.exp(k[i] - m), v[i])\n",
    "\n",
    "        return w / u\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Define the full model\n",
    "class ParallelAttentionClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ParallelAttentionClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "\n",
    "        # Learnable query\n",
    "        self.query = nn.Parameter(torch.randn(1, d_model))\n",
    "\n",
    "        # Parallel Attention Scan\n",
    "        self.attention_scan = ParallelAttentionScan()\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Expand query to match batch size\n",
    "        query = self.query.expand(batch_size, -1)\n",
    "\n",
    "        # Transpose x to match the expected input shape for ParallelAttentionScan\n",
    "        x = x.transpose(0, 1)  # Shape: (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Compute attention output\n",
    "        attention_output = self.attention_scan(query, x, x)  # Shape: (batch_size, d_model)\n",
    "\n",
    "        # Pass through the classification head\n",
    "        logits = self.fc(attention_output)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Simulate a more structured dataset\n",
    "class StructuredDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, d_model, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Simulate structured data (e.g., MFCCs)\n",
    "        self.data = torch.randn(num_samples, seq_len, d_model)  # Random audio frames\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))  # Random labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Configuration\n",
    "batch_size = 64\n",
    "seq_len = 20  # Number of audio frames per sequence\n",
    "d_model = 40  # Dimensionality of each audio frame (e.g., MFCCs)\n",
    "num_classes = 5  # Number of classes for classification\n",
    "num_samples = 1000  # Number of samples in the dataset\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = StructuredDataset(num_samples, seq_len, d_model, num_classes)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ParallelAttentionClassifier(d_model, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:  # Print every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], \"\n",
    "                  f\"Loss: {running_loss / 10:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_dataset = StructuredDataset(num_samples=200, seq_len=seq_len, d_model=d_model, num_classes=num_classes)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        logits = model(data)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewBasePy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Fig.2 : Attentionâ€™s RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x209cbf850b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AttentionRNNCell(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionRNNCell, self).__init__()\n",
    "        self.d_model = d_model  # Dimensionality of keys/queries/values\n",
    "    \n",
    "    def forward_iterative(self, q, k, v, prev_a=None, prev_c=None, prev_m=None):\n",
    "        \"\"\"\n",
    "        Perform a single step of the recurrent attention computation.\n",
    "        Args:\n",
    "            q (Tensor): Query vector, shape (batch_size, d_model).\n",
    "                - Corresponds to the query of the current time step.\n",
    "\n",
    "            k (Tensor): Key vector, shape (batch_size, d_model).\n",
    "            v (Tensor): Value vector, shape (batch_size, d_model).\n",
    "                - Corresponds to the values and keys from the first step to the current time step.\n",
    "\n",
    "\n",
    "            prev_a (Tensor): Previous a_k, shape (batch_size, d_model). Defaults to 0.\n",
    "            prev_c (Tensor): Previous c_k, shape (batch_size, 1). Defaults to 0.\n",
    "            prev_m (Tensor): Previous m_k, shape (batch_size, 1). Defaults to 0.\n",
    "        Returns:\n",
    "            a_k (Tensor), c_k (Tensor), m_k (Tensor)\n",
    "        \"\"\"\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # For debugging purposes\n",
    "        assert k.shape[-1] == v.shape[-1] == self.d_model\n",
    "\n",
    "        # Initialize previous states if not provided\n",
    "        if prev_a is None:\n",
    "            prev_a = torch.zeros(batch_size, self.d_model).to(k.device)\n",
    "        if prev_c is None:\n",
    "            prev_c = torch.zeros(batch_size, self.d_model).to(k.device)\n",
    "        if prev_m is None:\n",
    "            prev_m = torch.full((batch_size, 1), 0).to(k.device)\n",
    "\n",
    "        # Compute scores s_k = q . k^T\n",
    "        s_k = torch.sum(q * k, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update m_k (max cumulative score)\n",
    "        m_k = torch.max(s_k, prev_m)\n",
    "\n",
    "        # Compute exp terms for stability\n",
    "        exp_term1 = torch.exp(prev_m - m_k)  # Shape: (batch_size, 1)\n",
    "        exp_term2 = torch.exp(s_k - m_k)    # Shape: (batch_size, 1)\n",
    "\n",
    "        # Update a_k and c_k\n",
    "        a_k = prev_a * exp_term1 + v * exp_term2  # Shape: (batch_size, d_model)\n",
    "        c_k = prev_c * exp_term1 + exp_term2      # Shape: (batch_size, 1)\n",
    "\n",
    "        return a_k, c_k, m_k\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itereative Computation\n",
    "- Recurrently token-by-token (i.e., sequentially) in O(1)\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_NN_trainable_Model():\n",
    "    def __init__(self, model,batch_size, input_dim, d_model, optimizer):\n",
    "        self.d_model = d_model\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.input_dim = input_dim\n",
    "        self.model = AttentionRNNCell(d_model)\n",
    "    def forward_QKV():\n",
    "        \n",
    "\n",
    "        return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a_k: torch.Size([1, 2]) \n",
      "Final a_k:\n",
      " tensor([[-0.5620,  1.3149]])\n",
      "Shape of c_k: torch.Size([1, 2]) \n",
      "Final c_k:\n",
      " tensor([[4.0781, 4.0781]])\n",
      "Shape of m_k: torch.Size([1, 1]) \n",
      "Final m_k:\n",
      " tensor([[0.]])\n",
      "Final output:\n",
      " tensor([[-0.1378,  0.3224]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "batch_size = 1  # Number of sequences to process at once\n",
    "input_dim = 10  # Sequence length (N=10)\n",
    "d_model = 2     # Dimensionality of embeddings ( in text context), values representation\n",
    "\n",
    "# Dummy Data\n",
    "q = torch.randn(batch_size, d_model)  # Query vector\n",
    "k = torch.randn(input_dim, batch_size, d_model)  # Key matrix\n",
    "v = torch.randn(input_dim, batch_size, d_model)  # Value matrix\n",
    "\n",
    "# Initialize Cell\n",
    "cell = AttentionRNNCell(d_model)\n",
    "a_k, c_k, m_k = None, None, None\n",
    "\n",
    "# Process Iteratively\n",
    "for i in range(input_dim):\n",
    "    a_k, c_k, m_k = cell.forward_iterative(q, k[i, :, :], v[i, :, :], a_k, c_k, m_k)\n",
    "    \n",
    "import numpy as np\n",
    "if c_k != np.zeros((1,1)):\n",
    "    final_output = a_k/ c_k\n",
    "    \n",
    "print(\"Shape of a_k:\", a_k.shape, \"\\nFinal a_k:\\n\", a_k)\n",
    "print(\"Shape of c_k:\", c_k.shape, \"\\nFinal c_k:\\n\", c_k)\n",
    "print(\"Shape of m_k:\", m_k.shape, \"\\nFinal m_k:\\n\", m_k)\n",
    "print(\"Final output:\\n\", final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computation\n",
    "- Many-to-many RNN\n",
    "    -  we can compute\n",
    "{ak}N\n",
    "k=1, {ck}N\n",
    "k=1, and {mk}N\n",
    "k=1 via the parallel scan\n",
    "algorithm and \n",
    "**AFTERWARDS** combine ak and ck to compute Attention(q, x1:k).\n",
    "<html>\n",
    "\n",
    "### Image 1: MTM Diagram\n",
    "![MTM Diagram](MTM.png \"MTM Diagram Title\")\n",
    "\n",
    "### Image 2: Parallel Prefix Scan Diagram\n",
    "- This is the first intuation behind the implementation, yet we are going to use a litteral implementation of it as it is\n",
    "\n",
    "![Parallel Prefix Scan Diagram](ParallelPrefixScan.png \"Parallel Prefix Scan Diagram Title\")\n",
    "\n",
    "</html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3])\n",
      "Attention Output:\n",
      " tensor([[-0.1760, -0.0023,  0.0350],\n",
      "        [ 0.1519, -0.1678, -0.2512]])\n"
     ]
    }
   ],
   "source": [
    "# Define the parallel attention implementation as a module\n",
    "class ParallelAttentionScan(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParallelAttentionScan, self).__init__()\n",
    "\n",
    "    def combine(self, mA, uA, wA, mB, uB, wB):\n",
    "        mAB = torch.max(mA, mB)\n",
    "        expA = torch.exp(mA - mAB)\n",
    "        expB = torch.exp(mB - mAB)\n",
    "        uAB = uA * expA + uB * expB\n",
    "        wAB = wA * expA + wB * expB\n",
    "        return mAB, uAB, wAB\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        input_dim, batch_size, d_model = k.size()\n",
    "        # {(m{i}, u{i}, w{i})}N,i=1 == {(si, 1, vi)}N,i=1\n",
    "        \n",
    "        s_initial = torch.sum(q * k[0], dim=-1, keepdim=True)\n",
    "        m = s_initial\n",
    "        u = torch.ones(batch_size, 1)\n",
    "        w = v[0]\n",
    "\n",
    "        for i in range(1, input_dim):\n",
    "            m, u, w = self.combine(m, u, w, k[i], torch.exp(k[i] - m), v[i])\n",
    "\n",
    "        return w / u\n",
    "    \n",
    "# Example input tensors\n",
    "batch_size = 2\n",
    "input_dim = 5\n",
    "d_model = 3\n",
    "\n",
    "q = torch.randn(batch_size, d_model)  # Query vector (batch_size, d_model)\n",
    "k = torch.randn(input_dim, batch_size, d_model)  # Key matrix (input_dim, batch_size, d_model)\n",
    "v = torch.randn(input_dim, batch_size, d_model)  # Value matrix (input_dim, batch_size, d_model)\n",
    "\n",
    "# Run parallel attention scan\n",
    "ATT_parallel = ParallelAttentionScan()\n",
    "o = ATT_parallel(q, k, v)\n",
    "print(\"Output shape:\", o.shape)\n",
    "print(\"Attention Output:\\n\", o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative VS Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Output: tensor([[0.2147, 0.0529, 0.0588]])\n",
      "Iterative Output: tensor([[ 0.0920, -0.0360,  0.0753]])\n",
      "Absolute Error: tensor([[0.1227, 0.0890, 0.0165]])\n",
      "Relative Error: tensor([[1.3332, 2.4698, 0.2192]])\n",
      "Mean Absolute Error: tensor(0.0761)\n",
      "Mean Relative Error: tensor(1.3407)\n"
     ]
    }
   ],
   "source": [
    "# Seed for reproducibility  \n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuration\n",
    "batch_size = 1\n",
    "input_dim = 100\n",
    "d_model = 3\n",
    "\n",
    "q = torch.randn(batch_size, d_model)  # Query vector\n",
    "k = torch.randn(input_dim, batch_size, d_model)  # Key matrix\n",
    "v = torch.randn(input_dim, batch_size, d_model)  # Value matrix\n",
    "\n",
    "# Run parallel attention scan\n",
    "ATT_parallel = ParallelAttentionScan()\n",
    "parallel_output = ATT_parallel(q, k, v)\n",
    "\n",
    "# Initialize RNN Cell\n",
    "cell = AttentionRNNCell(d_model)\n",
    "a_k, c_k, m_k = None, None, None\n",
    "\n",
    "# Process Iteratively\n",
    "for i in range(input_dim):\n",
    "    a_k, c_k, m_k = cell.forward_iterative(q, k[i, :, :], v[i, :, :], a_k, c_k, m_k)\n",
    "\n",
    "iterative_output = a_k / c_k  # Normalize the result to get final attention output\n",
    "\n",
    "# Compute Absolute and Relative Errors\n",
    "absolute_error = torch.abs(parallel_output - iterative_output)\n",
    "relative_error = torch.abs(parallel_output - iterative_output) / torch.abs(iterative_output)\n",
    "\n",
    "print(\"Parallel Output:\", parallel_output)\n",
    "print(\"Iterative Output:\", iterative_output)\n",
    "\n",
    "print(\"Absolute Error:\", absolute_error)\n",
    "print(\"Relative Error:\", relative_error)\n",
    "\n",
    "print(\"Mean Absolute Error:\", torch.mean(absolute_error))\n",
    "print(\"Mean Relative Error:\", torch.mean(relative_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting into onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the models\n",
    "# parallel_model = ParallelAttentionScan()\n",
    "# iterative_model = IterativeAttentionScan(d_model=3)\n",
    "\n",
    "# # Dummy inputs\n",
    "# batch_size = 2\n",
    "# input_dim = 5\n",
    "# d_model = 3\n",
    "# q = torch.randn(batch_size, d_model)\n",
    "# k = torch.randn(input_dim, batch_size, d_model)\n",
    "# v = torch.randn(input_dim, batch_size, d_model)\n",
    "\n",
    "# # Export Parallel Attention Model to ONNX\n",
    "# torch.onnx.export(\n",
    "#     parallel_model,\n",
    "#     (q, k, v),\n",
    "#     \"parallel_attention_scan.onnx\",\n",
    "#     input_names=[\"q\", \"k\", \"v\"],\n",
    "#     output_names=[\"output\"],\n",
    "#     dynamic_axes={\"k\": {0: \"input_dim\"}, \"v\": {0: \"input_dim\"}},\n",
    "#     opset_version=12,\n",
    "# )\n",
    "\n",
    "# # Export Iterative Attention Model to ONNX\n",
    "# torch.onnx.export(\n",
    "#     iterative_model,\n",
    "#     (q, k, v),\n",
    "#     \"iterative_attention_scan.onnx\",\n",
    "#     input_names=[\"q\", \"k\", \"v\"],\n",
    "#     output_names=[\"output\"],\n",
    "#     dynamic_axes={\"k\": {0: \"input_dim\"}, \"v\": {0: \"input_dim\"}},\n",
    "#     opset_version=12,\n",
    "# )\n",
    "\n",
    "# print(\"Both models have been exported as ONNX.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX models Visualization\n",
    "### Image 1: Iterative Model\n",
    "![Iterative Model](itereative.png \"\")\n",
    "\n",
    "### Image 2: Parallel Model\n",
    "![Parallel Model](paralell.png \"\")\n",
    "\n",
    "</html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewBasePy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
